{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock ml libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root label (source = ASRS coding forms) : order = by descending frequency\n",
    "ANOMALY_LABELS = ['Deviation / Discrepancy - Procedural',\n",
    "                    'Aircraft Equipment',\n",
    "                    'Conflict',\n",
    "                    'Inflight Event / Encounter',\n",
    "                    'ATC Issue',\n",
    "                    'Deviation - Altitude',\n",
    "                    'Deviation - Track / Heading',\n",
    "                    'Ground Event / Encounter',\n",
    "                    'Flight Deck / Cabin / Aircraft Event',\n",
    "                    'Ground Incursion',\n",
    "                    'Airspace Violation',\n",
    "                    'Deviation - Speed',\n",
    "                    'Ground Excursion',\n",
    "                    'No Specific Anomaly Occurred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, labels, add_other=False):\n",
    "    loaded_data = pd.read_pickle(path)[0]\n",
    "\n",
    "    # Drop Anomaly NaN's\n",
    "    loaded_data = loaded_data.dropna(subset=['Anomaly']).reset_index(drop=True)\n",
    "\n",
    "    # Convert the 'Anomaly' column to a list of lists\n",
    "    anomaly_series = loaded_data['Anomaly']\n",
    "    anomaly_list = anomaly_series.str.split(';').apply(lambda x: [item.strip() for item in x])\n",
    "\n",
    "    # Initialize a DataFrame to hold the one-hot-encoded anomalies\n",
    "    anomaly_df = pd.DataFrame(index=loaded_data.index)\n",
    "\n",
    "    # Populate the DataFrame with one-hot-encoded columns for each prefix\n",
    "    for prefix in labels:\n",
    "        anomaly_df[prefix] = anomaly_list.apply(lambda anomalies: any(anomaly.startswith(prefix) for anomaly in anomalies)).astype(int)\n",
    "\n",
    "    # Add the 'Other' category\n",
    "    if add_other:\n",
    "        anomaly_df['Other'] = (anomaly_df.sum(axis=1) == 0).astype(int)\n",
    "\n",
    "    # Assign the one-hot-encoded anomalies as a new column 'labels' to 'loaded_data'\n",
    "    loaded_data['labels'] = anomaly_df.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "    # Now, 'loaded_data' is a DataFrame that includes both the 'text' and 'labels' columns\n",
    "    loaded_data['text'] = loaded_data[\"Narrative\"]\n",
    "\n",
    "    # If you want to create a new DataFrame with just 'text' and 'labels':\n",
    "    final_df = loaded_data[['text', 'labels']]\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop the NaN values in Anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was the pilot flying performing the takeoff....</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We had 6 shipments of dry ice for the flight; ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have seen a lot of mistakes on every flight ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was my first time flying into KEUG and I wa...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am writing this report to bring attention to...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96981</th>\n",
       "      <td>WE WERE ENRTE IN LNAV AT FL310; 30 MI N OF ATL...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96982</th>\n",
       "      <td>CLRED BY TWR CTL TO CROSS RWY 8R/26L AT TXWY E...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96983</th>\n",
       "      <td>WHILE WORKING NUMEROUS CVG AND CMH DEPS AT A C...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96984</th>\n",
       "      <td>ON MIDNIGHT SHIFT; APPROX XA00 LCL TIME; 2 SEC...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96985</th>\n",
       "      <td>I was working the FD/CD (Flight Data/Clearance...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96986 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      I was the pilot flying performing the takeoff....   \n",
       "1      We had 6 shipments of dry ice for the flight; ...   \n",
       "2      I have seen a lot of mistakes on every flight ...   \n",
       "3      It was my first time flying into KEUG and I wa...   \n",
       "4      I am writing this report to bring attention to...   \n",
       "...                                                  ...   \n",
       "96981  WE WERE ENRTE IN LNAV AT FL310; 30 MI N OF ATL...   \n",
       "96982  CLRED BY TWR CTL TO CROSS RWY 8R/26L AT TXWY E...   \n",
       "96983  WHILE WORKING NUMEROUS CVG AND CMH DEPS AT A C...   \n",
       "96984  ON MIDNIGHT SHIFT; APPROX XA00 LCL TIME; 2 SEC...   \n",
       "96985  I was working the FD/CD (Flight Data/Clearance...   \n",
       "\n",
       "                                           labels  \n",
       "0      [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2      [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "3      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                           ...  \n",
       "96981  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "96982  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "96983  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "96984  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "96985  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[96986 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df = load_data(\"./data/train_data_final.pkl\", ANOMALY_LABELS)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flying into SLC on the DELTA THREE RNAV arriva...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD was on a very busy east flow arrival push....</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B737-800 was vectored to an ILS Runway 16L app...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We were on a 6 mile final when tower cleared a...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During Climb we Leveled at 17;000 departure sw...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>FO was flying a visual approach to runway 26 i...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>While assembling a GE C2 transfer gearbox; I n...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>Nearing the end of a hot; bumpy four-hour IFR ...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10803</th>\n",
       "      <td>On approach gear went down and noticed yellow ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10804</th>\n",
       "      <td>Approximately 20 minutes into our Ferry Flight...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10805 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      Flying into SLC on the DELTA THREE RNAV arriva...   \n",
       "1      ORD was on a very busy east flow arrival push....   \n",
       "2      B737-800 was vectored to an ILS Runway 16L app...   \n",
       "3      We were on a 6 mile final when tower cleared a...   \n",
       "4      During Climb we Leveled at 17;000 departure sw...   \n",
       "...                                                  ...   \n",
       "10800  FO was flying a visual approach to runway 26 i...   \n",
       "10801  While assembling a GE C2 transfer gearbox; I n...   \n",
       "10802  Nearing the end of a hot; bumpy four-hour IFR ...   \n",
       "10803  On approach gear went down and noticed yellow ...   \n",
       "10804  Approximately 20 minutes into our Ferry Flight...   \n",
       "\n",
       "                                           labels  \n",
       "0      [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1      [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2      [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3      [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                           ...  \n",
       "10800  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10801  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10802  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10803  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10804  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "\n",
       "[10805 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_data(\"./data/test_data_final.pkl\", ANOMALY_LABELS)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = None\n",
    "MODEL_DIRECTORY = \"model_save\"\n",
    "\n",
    "\n",
    "# Sections of configBertTokenizer\n",
    "# Defining some key variables that will be used later on in the training\n",
    "BALANCED = False\n",
    "LAYERS_TO_UNFREEZE = None\n",
    "LAYERS_TO_UNFREEZE = [8, 9, 10, 11]\n",
    "# MAX_LEN = 512\n",
    "MAX_LEN = 4096\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = 32\n",
    "TRAIN_BATCH_SIZE = 16 # 32 Effective size for NASA\n",
    "ACCUMULATION_STEPS = TRAIN_EFFECTIVE_BATCH_SIZE / TRAIN_BATCH_SIZE\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 5 # 5 Epochs for NASA\n",
    "LEARNING_RATE = 1e-05 * 2 # 0.00002 Rate for NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text.iloc[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets.iloc[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=15):\n",
    "        super(SequenceClassificationModel, self).__init__()\n",
    "        self.original_name = model_name\n",
    "        self.model_name = model_name.replace(\"/\", \"_\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output = self.model(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        return output.logits\n",
    "    \n",
    "    def tokenizer(self):\n",
    "        return AutoTokenizer.from_pretrained(self.original_name)\n",
    "    \n",
    "    def _set_layer_trainable(self, layer, trainable):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def _find_and_set_encoder_layers(self, module, layer_nums, trainable):\n",
    "        if hasattr(module, 'encoder'):\n",
    "            for layer_num in layer_nums:\n",
    "                try:\n",
    "                    self._set_layer_trainable(module.encoder.layer[layer_num], trainable)\n",
    "                except IndexError:\n",
    "                    print(f\"Layer {layer_num} not found in the encoder.\")\n",
    "            return True\n",
    "        else:\n",
    "            for child in module.children():\n",
    "                if self._find_and_set_encoder_layers(child, layer_nums, trainable):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def set_trainable_layers(self, layer_nums=None):\n",
    "        if layer_nums is not None:\n",
    "            self.model_name += f'{self.model_name}_Unfrozen{layer_nums}'\n",
    "            \n",
    "        # Freeze all parameters first\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze classifier layers\n",
    "        if hasattr(self.model, 'classifier'):\n",
    "            self._set_layer_trainable(self.model.classifier, True)\n",
    "\n",
    "        # Attempt to find and unfreeze encoder layers\n",
    "        if not self._find_and_set_encoder_layers(self.model, layer_nums or [], True):\n",
    "            print(\"Encoder layers not found in the model.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassificationModel(\n",
       "  (model): LongformerForSequenceClassification(\n",
       "    (longformer): LongformerModel(\n",
       "      (embeddings): LongformerEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      )\n",
       "      (encoder): LongformerEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x LongformerLayer(\n",
       "            (attention): LongformerAttention(\n",
       "              (self): LongformerSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): LongformerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): LongformerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): LongformerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): LongformerClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=14, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(test_df.labels[0])\n",
    "\n",
    "# model = SequenceClassificationModel('bert-base-uncased', num_labels=num_labels)\n",
    "# model = SequenceClassificationModel('NASA-AIML/MIKA_SafeAeroBERT', num_labels=num_labels)\n",
    "model = SequenceClassificationModel('allenai/longformer-base-4096', num_labels=num_labels)\n",
    "\n",
    "model.set_trainable_layers(LAYERS_TO_UNFREEZE)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (96986, 2)\n",
      "TEST Dataset: (10805, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "tokenizer = model.tokenizer()\n",
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_per_label(y_true, y_pred, threshold=0.5):\n",
    "    preds = (y_pred >= threshold)\n",
    "    correct = preds == y_true\n",
    "    accuracy_per_label = correct.mean(axis=0)\n",
    "    return accuracy_per_label\n",
    "\n",
    "def binary_accuracy_averaged(y_true, y_pred, threshold=0.5):\n",
    "    accuracy_per_label = binary_accuracy_per_label(y_true, y_pred, threshold)\n",
    "    accuracy_averaged = accuracy_per_label.mean()\n",
    "    return accuracy_averaged\n",
    "\n",
    "def custom_classification_report(y_true, y_pred):\n",
    "    report = metrics.classification_report(y_true, y_pred, output_dict=True, labels=ANOMALY_LABELS, zero_division=0)\n",
    "    accuracy = binary_accuracy_per_label(y_true, y_pred)\n",
    "    extended_accuracy_new = np.append(accuracy, [accuracy.mean()] * (len(report) - len(accuracy)))\n",
    "\n",
    "    updated_report = {}\n",
    "    for i, class_label in enumerate(report.keys()):\n",
    "        # Create a new dictionary for the class with binary accuracy\n",
    "        class_dict = {'binary_accuracy': extended_accuracy_new[i]}\n",
    "        \n",
    "        # Merge this dictionary with the existing metrics for the class\n",
    "        class_dict.update(report[class_label])\n",
    "\n",
    "        # Update the main report dictionary\n",
    "        updated_report[class_label] = class_dict\n",
    "\n",
    "    return updated_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = None\n",
    "if BALANCED:\n",
    "    # Compute weights for loss function\n",
    "    num_labels = len(training_set[0]['targets'])\n",
    "    pos_num = torch.zeros(num_labels).to(device)\n",
    "    for _, data in enumerate(training_loader, 0):\n",
    "        targets = data['targets'].to(device)\n",
    "        pos_num += torch.sum(targets, axis=0)\n",
    "    nobs = len(training_loader.dataset)\n",
    "    pos_weight = (nobs - pos_num) / pos_num\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # compute weighted loss for unbalanced dataset\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "metrics_dict = {\"Custom Classifcation Report\": lambda y_true, y_pred: custom_classification_report(y_true, y_pred)\n",
    "    # \"Binary Accuracy Macro\": lambda outputs, targets: binary_accuracy_averaged(targets, outputs, threshold=0.5),\n",
    "    # \"Binary Accuracy per Class\": binary_accuracy_per_label,\n",
    "    # \"F1 Score Micro\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "    # \"F1 Score Macro\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "    # \"F1 Scores per Class\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average=None, zero_division=1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, directory='model_save', model_name=None):\n",
    "    \"\"\"\n",
    "    Saves the model state.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to save.\n",
    "    epoch (int): The current epoch number.\n",
    "    file_path (str): Base directory to save the models.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    file_path = os.path.join(directory, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f'Model saved at {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, directory='model_save', model_name=None, epoch=None):\n",
    "    \"\"\"\n",
    "    Loads the model state.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to load state into.\n",
    "    file_path (str): Path to the saved model file.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    if epoch is None:\n",
    "        epoch = find_last_saved_epoch(directory, model_name)\n",
    "        if epoch == -1:\n",
    "            print(\"No saved model found.\")\n",
    "            return\n",
    "    \n",
    "    file_path = os.path.join(directory, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"No model file found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    model.load_state_dict(torch.load(file_path))\n",
    "    model.to(device)\n",
    "    print(f'Model loaded from {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_saved_epoch(directory='model_save', model_name=None):\n",
    "    \"\"\"\n",
    "    Finds the last saved epoch number in the specified directory.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The directory where models are saved.\n",
    "\n",
    "    Returns:\n",
    "    int: The last saved epoch number. Returns -1 if no saved model is found.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(directory):\n",
    "        return -1\n",
    "\n",
    "    saved_epochs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if model_name is None or filename.startswith(model_name):\n",
    "            parts = filename.replace('.pth', '').split('_')\n",
    "            if parts[-2] == 'epoch':\n",
    "                try:\n",
    "                    saved_epochs.append(int(parts[-1]))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    return max(saved_epochs, default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, batch_data, device, loss_fn, mode, optimizer=None, accumulate_gradients=False):\n",
    "    ids = batch_data['ids'].to(device, dtype=torch.long)\n",
    "    mask = batch_data['mask'].to(device, dtype=torch.long)\n",
    "    token_type_ids = batch_data['token_type_ids'].to(device, dtype=torch.long)\n",
    "    targets = batch_data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "    if mode == 'train':\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        if not accumulate_gradients:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "    return outputs, targets, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_dict, y_true, y_pred):\n",
    "    results = {}\n",
    "    labels = anomaly_labels + [\"Other\"]\n",
    "    for metric_name, metric_fn in metrics_dict.items():\n",
    "        if metric_name == \"F1 Scores per Class\":\n",
    "            # Calculate F1 score for each class and associate with labels\n",
    "            f1_scores = metric_fn(y_true, y_pred)\n",
    "            for i, score in enumerate(f1_scores):\n",
    "                label = labels[i] if i < len(labels) else f\"Class {i}\"\n",
    "                results[f\"F1 Score - {label}\"] = score\n",
    "        elif metric_name == \"Binary Accuracy per Class\":\n",
    "            bin_acc = metric_fn(y_true, y_pred)\n",
    "            for i, score in enumerate(bin_acc):\n",
    "                label = labels[i] if i < len(labels) else f\"Class {i}\"\n",
    "                results[f\"Binary Accuracy - {label}\"] = score\n",
    "        else:\n",
    "            results[metric_name] = metric_fn(y_true, y_pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(val):\n",
    "    \"\"\"Helper function to format the value for printing.\"\"\"\n",
    "    return f\"{val:.4f}\" if isinstance(val, (float, np.float64)) else val\n",
    "\n",
    "def print_metrics_results(metrics_results):\n",
    "    # First, print scalar values and simple dictionaries\n",
    "    for metric, value in metrics_results.items():\n",
    "        if isinstance(value, dict) and not any(isinstance(v, dict) for v in value.values()):\n",
    "            # Print simple dictionaries on a single line\n",
    "            dict_values = \", \".join([f\"{k}: {format_value(v)}\" for k, v in value.items()])\n",
    "            print(f\"{metric}: {dict_values}\")\n",
    "        elif not isinstance(value, dict):\n",
    "            # Print scalar values\n",
    "            print(f\"{metric}: {format_value(value)}\")\n",
    "\n",
    "    # Then, print nested dictionaries\n",
    "    for metric, value in metrics_results.items():\n",
    "        if isinstance(value, dict) and any(isinstance(v, dict) for v in value.values()):\n",
    "            # Print nested dictionaries\n",
    "            print(f\"\\n{metric}:\")\n",
    "            # Find the longest key length for formatting\n",
    "            max_key_length = max(len(str(k)) for k in value.keys())\n",
    "            for sub_key, sub_dict in value.items():\n",
    "                formatted_key = f\"{sub_key}:\".ljust(max_key_length + 2)\n",
    "                dict_values = \", \".join([f\"{k}: {format_value(v)}\" for k, v in sub_dict.items()])\n",
    "                print(f\"  {formatted_key} {dict_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_results(mode, epoch, batch, dataset_size, loss, start_time, batch_start_time, batch_size):\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    batch_time_ms = (current_time - batch_start_time) * 1000\n",
    "\n",
    "    current = (batch + 1) * batch_size\n",
    "    epoch_str = f\"Epoch: {epoch+1}, \" if epoch is not None else \"\"\n",
    "    \n",
    "    print(f\"\\r{mode.capitalize()} - {epoch_str}Batch: {batch+1} [{current:>5d}/{dataset_size:>5d}], \"\n",
    "          f\"Time: {elapsed_time:.0f}s {batch_time_ms:.0f}ms/step, Loss: {loss:>7f}\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(mode, model, loader, device, loss_fn, optimizer=None, epoch=None, accumulation_steps=None):\n",
    "    total_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, data in enumerate(loader, 0):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        logits, targets, loss = process_batch(model, data, device, loss_fn, mode, optimizer)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if mode == 'train':\n",
    "            if accumulation_steps is not None and (batch + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        outputs_binary = torch.sigmoid(logits).cpu().detach().numpy() >= 0.5\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        all_outputs.extend(outputs_binary)\n",
    "        all_targets.extend(targets)\n",
    "\n",
    "        batch_size = targets.shape[0]\n",
    "        print_batch_results(mode, epoch, batch, len(loader.dataset), loss.item(), start_time, batch_start_time, batch_size)\n",
    "\n",
    "    if mode == 'train' and optimizer is not None and accumulation_steps is not None:\n",
    "        # Ensure any remaining gradients are applied\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "\n",
    "    print()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, all_outputs, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, loss_fn, metrics_dict, device):\n",
    "    model.eval()\n",
    "    avg_val_loss, val_outputs, val_targets = process_batches('evaluate', model, validation_loader, device, loss_fn)\n",
    "    \n",
    "    metrics_results = calculate_metrics(metrics_dict, val_targets, val_outputs)\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Average Loss: {avg_val_loss:.4f}\")\n",
    "    print_metrics_results(metrics_results)\n",
    "\n",
    "    return avg_val_loss, metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, training_loader, validation_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps=1):\n",
    "    print(f\"Training Epoch {epoch + 1}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    if optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss, train_outputs, train_targets = process_batches('train', model, training_loader, device, loss_fn, optimizer, epoch, accumulation_steps)\n",
    "    print(f\"Average Training Loss for Epoch {epoch + 1}: {avg_train_loss:.4f}\")\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, train_targets, train_outputs)\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Average Loss: {avg_val_loss:.4f}\")\n",
    "    print_metrics_results(metrics_results)\n",
    "\n",
    "    # Validation phase\n",
    "    if validation_loader is not None:\n",
    "        avg_val_loss, val_metrics_results = evaluate(model, validation_loader, loss_fn, metrics_dict, device)\n",
    "    else:\n",
    "        avg_val_loss = None\n",
    "        val_metrics_results = {}\n",
    "\n",
    "    return avg_train_loss, avg_val_loss, val_metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found.\n",
      "Resuming training from epoch 1\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 5.80 GiB of which 95.50 MiB is free. Including non-PyTorch memory, this process has 5.69 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 377.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/gonem/ENSAI/Safran Project/aviation-classification.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResuming training from epoch \u001b[39m\u001b[39m{\u001b[39;00mstart_epoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_loss, val_loss, val_metrics \u001b[39m=\u001b[39m train(model, epoch, training_loader, testing_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     save_model(model, epoch, directory\u001b[39m=\u001b[39mMODEL_DIRECTORY, model_name\u001b[39m=\u001b[39mMODEL_NAME)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Additional epoch-level processing if needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Testing phase\u001b[39;00m\n",
      "\u001b[1;32m/home/gonem/ENSAI/Safran Project/aviation-classification.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m optimizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m avg_train_loss, train_outputs, train_targets \u001b[39m=\u001b[39m process_batches(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, model, training_loader, device, loss_fn, optimizer, epoch, accumulation_steps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage Training Loss for Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mavg_train_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m metrics_results \u001b[39m=\u001b[39m calculate_metrics(metrics_dict, train_targets, train_outputs)\n",
      "\u001b[1;32m/home/gonem/ENSAI/Safran Project/aviation-classification.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader, \u001b[39m0\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     batch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     logits, targets, loss \u001b[39m=\u001b[39m process_batch(model, data, device, loss_fn, mode, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;32m/home/gonem/ENSAI/Safran Project/aviation-classification.ipynb Cell 28\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m targets \u001b[39m=\u001b[39m batch_data[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(ids, mask, token_type_ids)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/gonem/ENSAI/Safran Project/aviation-classification.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, ids, mask, token_type_ids):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(ids, attention_mask\u001b[39m=\u001b[39;49mmask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/Safran%20Project/aviation-classification.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1925\u001b[0m, in \u001b[0;36mLongformerForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1922\u001b[0m     \u001b[39m# global attention on cls token\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m     global_attention_mask[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1925\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlongformer(\n\u001b[1;32m   1926\u001b[0m     input_ids,\n\u001b[1;32m   1927\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1928\u001b[0m     global_attention_mask\u001b[39m=\u001b[39;49mglobal_attention_mask,\n\u001b[1;32m   1929\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1930\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1931\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1932\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1933\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1934\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1935\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1936\u001b[0m )\n\u001b[1;32m   1937\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1938\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1738\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1730\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[1;32m   1731\u001b[0m     :, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :\n\u001b[1;32m   1732\u001b[0m ]\n\u001b[1;32m   1734\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1735\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, token_type_ids\u001b[39m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds\n\u001b[1;32m   1736\u001b[0m )\n\u001b[0;32m-> 1738\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1739\u001b[0m     embedding_output,\n\u001b[1;32m   1740\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1741\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1742\u001b[0m     padding_len\u001b[39m=\u001b[39;49mpadding_len,\n\u001b[1;32m   1743\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1744\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1745\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1746\u001b[0m )\n\u001b[1;32m   1747\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1748\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1318\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1308\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1309\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         output_attentions,\n\u001b[1;32m   1316\u001b[0m     )\n\u001b[1;32m   1317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1318\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1319\u001b[0m         hidden_states,\n\u001b[1;32m   1320\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1321\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mhead_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1322\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[1;32m   1323\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[1;32m   1324\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[1;32m   1325\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1326\u001b[0m     )\n\u001b[1;32m   1327\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1329\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1330\u001b[0m     \u001b[39m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1246\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   1237\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1238\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1245\u001b[0m ):\n\u001b[0;32m-> 1246\u001b[0m     self_attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m   1247\u001b[0m         hidden_states,\n\u001b[1;32m   1248\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1249\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1250\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[1;32m   1251\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[1;32m   1252\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[1;32m   1253\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m     attn_output \u001b[39m=\u001b[39m self_attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m     outputs \u001b[39m=\u001b[39m self_attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1182\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   1173\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1174\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1181\u001b[0m ):\n\u001b[0;32m-> 1182\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m   1183\u001b[0m         hidden_states,\n\u001b[1;32m   1184\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1185\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1186\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[1;32m   1187\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[1;32m   1188\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[1;32m   1189\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1190\u001b[0m     )\n\u001b[1;32m   1191\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[39m=\u001b[39m (attn_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:651\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39m# compute local attention output with global attention value and add\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m is_global_attn:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# compute sum of global and local attn\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_attn_output_with_global_indices(\n\u001b[1;32m    652\u001b[0m         value_vectors\u001b[39m=\u001b[39;49mvalue_vectors,\n\u001b[1;32m    653\u001b[0m         attn_probs\u001b[39m=\u001b[39;49mattn_probs,\n\u001b[1;32m    654\u001b[0m         max_num_global_attn_indices\u001b[39m=\u001b[39;49mmax_num_global_attn_indices,\n\u001b[1;32m    655\u001b[0m         is_index_global_attn_nonzero\u001b[39m=\u001b[39;49mis_index_global_attn_nonzero,\n\u001b[1;32m    656\u001b[0m         is_local_index_global_attn_nonzero\u001b[39m=\u001b[39;49mis_local_index_global_attn_nonzero,\n\u001b[1;32m    657\u001b[0m     )\n\u001b[1;32m    658\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[39m# compute local attn only\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sliding_chunks_matmul_attn_probs_value(\n\u001b[1;32m    661\u001b[0m         attn_probs, value_vectors, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_sided_attn_window_size\n\u001b[1;32m    662\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1018\u001b[0m, in \u001b[0;36mLongformerSelfAttention._compute_attn_output_with_global_indices\u001b[0;34m(self, value_vectors, attn_probs, max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero)\u001b[0m\n\u001b[1;32m   1013\u001b[0m attn_probs_without_global \u001b[39m=\u001b[39m attn_probs\u001b[39m.\u001b[39mnarrow(\n\u001b[1;32m   1014\u001b[0m     \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, max_num_global_attn_indices, attn_probs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m max_num_global_attn_indices\n\u001b[1;32m   1015\u001b[0m )\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m   1017\u001b[0m \u001b[39m# compute attn output with global\u001b[39;00m\n\u001b[0;32m-> 1018\u001b[0m attn_output_without_global \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sliding_chunks_matmul_attn_probs_value(\n\u001b[1;32m   1019\u001b[0m     attn_probs_without_global, value_vectors, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_sided_attn_window_size\n\u001b[1;32m   1020\u001b[0m )\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m attn_output_only_global \u001b[39m+\u001b[39m attn_output_without_global\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:923\u001b[0m, in \u001b[0;36mLongformerSelfAttention._sliding_chunks_matmul_attn_probs_value\u001b[0;34m(self, attn_probs, value, window_overlap)\u001b[0m\n\u001b[1;32m    915\u001b[0m chunked_value_stride \u001b[39m=\u001b[39m (\n\u001b[1;32m    916\u001b[0m     chunked_value_stride[\u001b[39m0\u001b[39m],\n\u001b[1;32m    917\u001b[0m     window_overlap \u001b[39m*\u001b[39m chunked_value_stride[\u001b[39m1\u001b[39m],\n\u001b[1;32m    918\u001b[0m     chunked_value_stride[\u001b[39m1\u001b[39m],\n\u001b[1;32m    919\u001b[0m     chunked_value_stride[\u001b[39m2\u001b[39m],\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m chunked_value \u001b[39m=\u001b[39m padded_value\u001b[39m.\u001b[39mas_strided(size\u001b[39m=\u001b[39mchunked_value_size, stride\u001b[39m=\u001b[39mchunked_value_stride)\n\u001b[0;32m--> 923\u001b[0m chunked_attn_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pad_and_diagonalize(chunked_attn_probs)\n\u001b[1;32m    925\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbcwd,bcdh->bcwh\u001b[39m\u001b[39m\"\u001b[39m, (chunked_attn_probs, chunked_value))\n\u001b[1;32m    926\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39mview(batch_size, num_heads, seq_len, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/SAFRAN/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:746\u001b[0m, in \u001b[0;36mLongformerSelfAttention._pad_and_diagonalize\u001b[0;34m(chunked_hidden_states)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39mshift every row 1 step right, converting columns into diagonals.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39m               -0.0405, 0.1599, 0.0000 0.0000, 0.0000, 0.0000, 2.0514, -1.1600, 0.5372, 0.2629 ]\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    745\u001b[0m total_num_heads, num_chunks, window_overlap, hidden_dim \u001b[39m=\u001b[39m chunked_hidden_states\u001b[39m.\u001b[39msize()\n\u001b[0;32m--> 746\u001b[0m chunked_hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    747\u001b[0m     chunked_hidden_states, (\u001b[39m0\u001b[39;49m, window_overlap \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    748\u001b[0m )  \u001b[39m# total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten\u001b[39;00m\n\u001b[1;32m    749\u001b[0m chunked_hidden_states \u001b[39m=\u001b[39m chunked_hidden_states\u001b[39m.\u001b[39mview(\n\u001b[1;32m    750\u001b[0m     total_num_heads, num_chunks, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    751\u001b[0m )  \u001b[39m# total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap\u001b[39;00m\n\u001b[1;32m    752\u001b[0m chunked_hidden_states \u001b[39m=\u001b[39m chunked_hidden_states[\n\u001b[1;32m    753\u001b[0m     :, :, :\u001b[39m-\u001b[39mwindow_overlap\n\u001b[1;32m    754\u001b[0m ]  \u001b[39m# total_num_heads x num_chunks x window_overlap*window_overlap\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacty of 5.80 GiB of which 95.50 MiB is free. Including non-PyTorch memory, this process has 5.69 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 377.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "last_saved_epoch = find_last_saved_epoch(directory=MODEL_DIRECTORY, model_name=MODEL_NAME)\n",
    "\n",
    "start_epoch = last_saved_epoch + 1 if last_saved_epoch != -1 else 0\n",
    "if last_saved_epoch != -1:\n",
    "    load_model(model, directory=MODEL_DIRECTORY, model_name=MODEL_NAME, epoch=last_saved_epoch)\n",
    "    print(f\"Loaded model training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"No saved model found.\")\n",
    "\n",
    "if start_epoch < EPOCHS:\n",
    "    print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    train_loss, val_loss, val_metrics = train(model, epoch, training_loader, testing_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps=8)\n",
    "    save_model(model, epoch, directory=MODEL_DIRECTORY, model_name=MODEL_NAME)\n",
    "    # Additional epoch-level processing if needed\n",
    "\n",
    "# Testing phase\n",
    "avg_test_loss, test_metrics_results = evaluate(model, testing_loader, loss_fn, metrics_dict, device)\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Average Loss: {avg_test_loss:.4f}\")\n",
    "print_metrics_results(test_metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
