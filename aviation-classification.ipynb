{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kornia\n",
    "# ! pip install kornia[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock ml libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from kornia.losses import BinaryFocalLossWithLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root label (source = ASRS coding forms) : order = by descending frequency\n",
    "ANOMALY_LABELS = ['Deviation / Discrepancy - Procedural',\n",
    "                    'Aircraft Equipment',\n",
    "                    'Conflict',\n",
    "                    'Inflight Event / Encounter',\n",
    "                    'ATC Issue',\n",
    "                    'Deviation - Altitude',\n",
    "                    'Deviation - Track / Heading',\n",
    "                    'Ground Event / Encounter',\n",
    "                    'Flight Deck / Cabin / Aircraft Event',\n",
    "                    'Ground Incursion',\n",
    "                    'Airspace Violation',\n",
    "                    'Deviation - Speed',\n",
    "                    'Ground Excursion',\n",
    "                    'No Specific Anomaly Occurred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, labels, add_other=False):\n",
    "    loaded_data = pd.read_pickle(path)[0]\n",
    "\n",
    "    # Drop Anomaly NaN's\n",
    "    loaded_data = loaded_data.dropna(subset=['Anomaly']).reset_index(drop=True)\n",
    "\n",
    "    # Convert the 'Anomaly' column to a list of lists\n",
    "    anomaly_series = loaded_data['Anomaly']\n",
    "    anomaly_list = anomaly_series.str.split(';').apply(lambda x: [item.strip() for item in x])\n",
    "\n",
    "    # Initialize a DataFrame to hold the one-hot-encoded anomalies\n",
    "    anomaly_df = pd.DataFrame(index=loaded_data.index)\n",
    "\n",
    "    # Populate the DataFrame with one-hot-encoded columns for each prefix\n",
    "    for prefix in labels:\n",
    "        anomaly_df[prefix] = anomaly_list.apply(lambda anomalies: any(anomaly.startswith(prefix) for anomaly in anomalies)).astype(int)\n",
    "\n",
    "    # Add the 'Other' category\n",
    "    if add_other:\n",
    "        anomaly_df['Other'] = (anomaly_df.sum(axis=1) == 0).astype(int)\n",
    "\n",
    "    # Assign the one-hot-encoded anomalies as a new column 'labels' to 'loaded_data'\n",
    "    loaded_data['labels'] = anomaly_df.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "    # Now, 'loaded_data' is a DataFrame that includes both the 'text' and 'labels' columns\n",
    "    loaded_data['text'] = loaded_data[\"Narrative\"]\n",
    "\n",
    "    # If you want to create a new DataFrame with just 'text' and 'labels':\n",
    "    final_df = loaded_data[['text', 'labels']]\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop the NaN values in Anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was the pilot flying performing the takeoff....</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We had 6 shipments of dry ice for the flight; ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have seen a lot of mistakes on every flight ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was my first time flying into KEUG and I wa...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am writing this report to bring attention to...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96981</th>\n",
       "      <td>WE WERE ENRTE IN LNAV AT FL310; 30 MI N OF ATL...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96982</th>\n",
       "      <td>CLRED BY TWR CTL TO CROSS RWY 8R/26L AT TXWY E...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96983</th>\n",
       "      <td>WHILE WORKING NUMEROUS CVG AND CMH DEPS AT A C...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96984</th>\n",
       "      <td>ON MIDNIGHT SHIFT; APPROX XA00 LCL TIME; 2 SEC...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96985</th>\n",
       "      <td>I was working the FD/CD (Flight Data/Clearance...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96986 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      I was the pilot flying performing the takeoff....   \n",
       "1      We had 6 shipments of dry ice for the flight; ...   \n",
       "2      I have seen a lot of mistakes on every flight ...   \n",
       "3      It was my first time flying into KEUG and I wa...   \n",
       "4      I am writing this report to bring attention to...   \n",
       "...                                                  ...   \n",
       "96981  WE WERE ENRTE IN LNAV AT FL310; 30 MI N OF ATL...   \n",
       "96982  CLRED BY TWR CTL TO CROSS RWY 8R/26L AT TXWY E...   \n",
       "96983  WHILE WORKING NUMEROUS CVG AND CMH DEPS AT A C...   \n",
       "96984  ON MIDNIGHT SHIFT; APPROX XA00 LCL TIME; 2 SEC...   \n",
       "96985  I was working the FD/CD (Flight Data/Clearance...   \n",
       "\n",
       "                                           labels  \n",
       "0      [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2      [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "3      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                           ...  \n",
       "96981  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "96982  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "96983  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "96984  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "96985  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[96986 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df = load_data(\"./data/train_data_final.pkl\", ANOMALY_LABELS)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flying into SLC on the DELTA THREE RNAV arriva...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD was on a very busy east flow arrival push....</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B737-800 was vectored to an ILS Runway 16L app...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We were on a 6 mile final when tower cleared a...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During Climb we Leveled at 17;000 departure sw...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>FO was flying a visual approach to runway 26 i...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>While assembling a GE C2 transfer gearbox; I n...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>Nearing the end of a hot; bumpy four-hour IFR ...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10803</th>\n",
       "      <td>On approach gear went down and noticed yellow ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10804</th>\n",
       "      <td>Approximately 20 minutes into our Ferry Flight...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10805 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      Flying into SLC on the DELTA THREE RNAV arriva...   \n",
       "1      ORD was on a very busy east flow arrival push....   \n",
       "2      B737-800 was vectored to an ILS Runway 16L app...   \n",
       "3      We were on a 6 mile final when tower cleared a...   \n",
       "4      During Climb we Leveled at 17;000 departure sw...   \n",
       "...                                                  ...   \n",
       "10800  FO was flying a visual approach to runway 26 i...   \n",
       "10801  While assembling a GE C2 transfer gearbox; I n...   \n",
       "10802  Nearing the end of a hot; bumpy four-hour IFR ...   \n",
       "10803  On approach gear went down and noticed yellow ...   \n",
       "10804  Approximately 20 minutes into our Ferry Flight...   \n",
       "\n",
       "                                           labels  \n",
       "0      [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1      [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2      [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3      [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "4      [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                           ...  \n",
       "10800  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10801  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10802  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10803  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10804  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "\n",
       "[10805 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_data(\"./data/test_data_final.pkl\", ANOMALY_LABELS)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = None\n",
    "MODEL_DIRECTORY = \"model_save\"\n",
    "\n",
    "\n",
    "# Sections of configBertTokenizer\n",
    "# Defining some key variables that will be used later on in the training\n",
    "LOSS_TYPE = 'BCE'\n",
    "# LOSS_TYPE = 'BinaryFocal'\n",
    "\n",
    "# BALANCED = False\n",
    "BALANCED = True\n",
    "\n",
    "# LAYERS_TO_UNFREEZE = None\n",
    "LAYERS_TO_UNFREEZE = [8, 9, 10, 11]\n",
    "\n",
    "# ENCODER_NAME = 'bert-base-uncased'\n",
    "# ENCODER_NAME = 'NASA-AIML/MIKA_SafeAeroBERT'\n",
    "ENCODER_NAME = 'allenai/longformer-base-4096'\n",
    "\n",
    "MAX_LEN = 512\n",
    "# MAX_LEN = 1024\n",
    "\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = 32 # 32 Effective size for NASA\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "ACCUMULATION_STEPS = TRAIN_EFFECTIVE_BATCH_SIZE / TRAIN_BATCH_SIZE\n",
    "VALID_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "EPOCHS = 5 # 5 Epochs for NASA\n",
    "LEARNING_RATE = 1e-05 * 2 # 0.00002 Rate for NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text.iloc[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets.iloc[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=15):\n",
    "        super(SequenceClassificationModel, self).__init__()\n",
    "        self.original_name = model_name\n",
    "        self.model_name = model_name.replace(\"/\", \"_\")\n",
    "        self.l1 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        return output.logits\n",
    "    \n",
    "    def tokenizer(self):\n",
    "        return AutoTokenizer.from_pretrained(self.original_name)\n",
    "    \n",
    "    def _set_layer_trainable(self, layer, trainable):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def _find_and_set_encoder_layers(self, module, layer_nums, trainable):\n",
    "        if hasattr(module, 'encoder'):\n",
    "            for layer_num in layer_nums:\n",
    "                try:\n",
    "                    self._set_layer_trainable(module.encoder.layer[layer_num], trainable)\n",
    "                except IndexError:\n",
    "                    print(f\"Layer {layer_num} not found in the encoder.\")\n",
    "            return True\n",
    "        else:\n",
    "            for child in module.children():\n",
    "                if self._find_and_set_encoder_layers(child, layer_nums, trainable):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def set_trainable_layers(self, layer_nums=None):\n",
    "        if layer_nums is not None:\n",
    "            self.model_name = f'{self.model_name}_Unfrozen{layer_nums}'\n",
    "            \n",
    "        # Freeze all parameters first\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze classifier layers\n",
    "        if hasattr(self.l1, 'classifier'):\n",
    "            self._set_layer_trainable(self.l1.classifier, True)\n",
    "\n",
    "        # Attempt to find and unfreeze encoder layers\n",
    "        if not self._find_and_set_encoder_layers(self.l1, layer_nums or [], True):\n",
    "            print(\"Encoder layers not found in the model.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassificationModel(\n",
       "  (l1): LongformerForSequenceClassification(\n",
       "    (longformer): LongformerModel(\n",
       "      (embeddings): LongformerEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      )\n",
       "      (encoder): LongformerEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x LongformerLayer(\n",
       "            (attention): LongformerAttention(\n",
       "              (self): LongformerSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): LongformerSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): LongformerIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): LongformerOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): LongformerClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=14, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(test_df.labels[0])\n",
    "model = SequenceClassificationModel(ENCODER_NAME, num_labels=num_labels)\n",
    "\n",
    "model.set_trainable_layers(LAYERS_TO_UNFREEZE)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (96986, 2)\n",
      "TEST Dataset: (10805, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "tokenizer = model.tokenizer()\n",
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_per_label(y_true, y_pred):\n",
    "    correct = y_pred == y_true\n",
    "    accuracy_per_label = correct.float().mean(axis=0)\n",
    "    return accuracy_per_label\n",
    "\n",
    "def binary_accuracy_averaged(y_true, y_pred):\n",
    "    accuracy_per_label = binary_accuracy_per_label(y_true, y_pred)\n",
    "    accuracy_averaged = accuracy_per_label.mean()\n",
    "    return accuracy_averaged\n",
    "\n",
    "def custom_classification_report(y_true, y_pred):\n",
    "    report = metrics.classification_report(y_true, y_pred, output_dict=True, target_names=ANOMALY_LABELS, zero_division=0)\n",
    "    accuracy = binary_accuracy_per_label(y_true, y_pred)\n",
    "    extended_accuracy_new = np.append(accuracy, [accuracy.mean()] * (len(report) - len(accuracy)))\n",
    "\n",
    "    updated_report = {}\n",
    "    for i, class_label in enumerate(report.keys()):\n",
    "        # Create a new dictionary for the class with binary accuracy\n",
    "        class_dict = {'binary_accuracy': extended_accuracy_new[i]}\n",
    "        \n",
    "        # Merge this dictionary with the existing metrics for the class\n",
    "        class_dict.update(report[class_label])\n",
    "\n",
    "        # Update the main report dictionary\n",
    "        updated_report[class_label] = class_dict\n",
    "\n",
    "    return updated_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, name='BCE', balance=False, dataset=None, dataloader=None, device='cpu'):\n",
    "    model.model_name += f'_{name}'\n",
    "    pos_weight = None\n",
    "    \n",
    "    if balance:\n",
    "        if dataset is None or dataloader is None:\n",
    "            raise ValueError(\"balance is set to True, but the data is not given\")\n",
    "        \n",
    "        # Compute weights for loss function\n",
    "        num_labels = len(dataset[0]['targets'])\n",
    "        pos_num = torch.zeros(num_labels).to(device)\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            targets = data['targets'].to(device)\n",
    "            pos_num += torch.sum(targets, axis=0)\n",
    "        nobs = len(dataloader.dataset)\n",
    "        pos_weight = (nobs - pos_num) / pos_num\n",
    "\n",
    "        model.model_name += \"-Balanced\"\n",
    "    \n",
    "    if name == 'BCE':\n",
    "        return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    elif name == 'BinaryFocal':\n",
    "        return BinaryFocalLossWithLogits(pos_weight=pos_weight, gamma=0.5, alpha=1, reduction='mean')\n",
    "    else:\n",
    "        raise ValueError(\"loss not known\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = loss(model, LOSS_TYPE, BALANCED, training_set, training_loader, device)\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "metrics_dict = {\"Custom Classifcation Report\": lambda y_true, y_pred: custom_classification_report(y_true, y_pred)\n",
    "    # \"Binary Accuracy Macro\": lambda outputs, targets: binary_accuracy_averaged(targets, outputs, threshold=0.5),\n",
    "    # \"Binary Accuracy per Class\": binary_accuracy_per_label,\n",
    "    # \"F1 Score Micro\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "    # \"F1 Score Macro\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "    # \"F1 Scores per Class\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average=None, zero_division=1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, directory='model_save', model_name=None):\n",
    "    \"\"\"\n",
    "    Saves the model state.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to save.\n",
    "    epoch (int): The current epoch number.\n",
    "    file_path (str): Base directory to save the models.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    file_path = os.path.join(directory, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f'Model saved at {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, directory='model_save', model_name=None, epoch=None):\n",
    "    \"\"\"\n",
    "    Loads the model state.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to load state into.\n",
    "    file_path (str): Path to the saved model file.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    if epoch is None:\n",
    "        epoch = find_last_saved_epoch(directory, model_name)\n",
    "        if epoch == -1:\n",
    "            print(\"No saved model found.\")\n",
    "            return\n",
    "    \n",
    "    file_path = os.path.join(directory, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"No model file found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    model.load_state_dict(torch.load(file_path))\n",
    "    model.to(device)\n",
    "    print(f'Model loaded from {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_saved_epoch(directory='model_save', model_name=None):\n",
    "    \"\"\"\n",
    "    Finds the last saved epoch number in the specified directory.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The directory where models are saved.\n",
    "\n",
    "    Returns:\n",
    "    int: The last saved epoch number. Returns -1 if no saved model is found.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(directory):\n",
    "        return -1\n",
    "\n",
    "    saved_epochs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if model_name is None or filename.startswith(model_name):\n",
    "            parts = filename.replace('.pth', '').split('_')\n",
    "            if parts[-2] == 'epoch':\n",
    "                try:\n",
    "                    saved_epochs.append(int(parts[-1]))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    return max(saved_epochs, default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, batch_data, device, loss_fn, mode, optimizer=None, accumulate_gradients=False):\n",
    "    ids = batch_data['ids'].to(device, dtype=torch.long)\n",
    "    mask = batch_data['mask'].to(device, dtype=torch.long)\n",
    "    token_type_ids = batch_data['token_type_ids'].to(device, dtype=torch.long)\n",
    "    targets = batch_data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "    if mode == 'train':\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        if not accumulate_gradients:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "    return outputs, targets, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_dict, targets, outputs, is_logit=True, thresholds=0.5, percentile=None):\n",
    "    results = {}\n",
    "    labels = ANOMALY_LABELS\n",
    "    if is_logit:\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "\n",
    "    if thresholds is None:\n",
    "        thresholds = 0.5\n",
    "    # Calculate percentile is specified\n",
    "    if percentile is not None:\n",
    "        thresholds = []\n",
    "        for i in range(outputs.shape[1]):  # Iterate over each label\n",
    "            label_scores = outputs[:, i].detach().cpu().numpy()\n",
    "            threshold = np.percentile(label_scores, percentile)\n",
    "            thresholds.append(threshold)\n",
    "        thresholds = np.array(thresholds)\n",
    "\n",
    "    # Apply thresholds to outputs\n",
    "    outputs = (outputs >= torch.tensor(thresholds, device=outputs.device)).float()\n",
    "\n",
    "    for metric_name, metric_fn in metrics_dict.items():\n",
    "        if metric_name in [\"F1 Scores per Class\", \"Binary Accuracy per Class\"]:\n",
    "            metric_scores = metric_fn(targets.cpu(), outputs.cpu())  # Assuming targets and outputs are tensors\n",
    "            for i, score in enumerate(metric_scores):\n",
    "                label = labels[i] if i < len(labels) else f\"Class {i}\"\n",
    "                results[f\"{metric_name} - {label}\"] = score\n",
    "        else:\n",
    "            results[metric_name] = metric_fn(targets.cpu(), outputs.cpu())\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(val):\n",
    "    \"\"\"Helper function to format the value for printing.\"\"\"\n",
    "    if isinstance(val, (float, np.float16, np.float32, np.float64)):\n",
    "        return f\"{val:.4f}\"\n",
    "    elif isinstance(val, torch.Tensor) and val.dtype in [torch.float16, torch.float32, torch.float64]:\n",
    "        return f\"{val.item():.4f}\"\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def print_metrics_results(metrics_results):\n",
    "    # First, print scalar values and simple dictionaries\n",
    "    for metric, value in metrics_results.items():\n",
    "        if isinstance(value, dict) and not any(isinstance(v, dict) for v in value.values()):\n",
    "            # Print simple dictionaries on a single line\n",
    "            dict_values = \", \".join([f\"{k}: {format_value(v)}\" for k, v in value.items()])\n",
    "            print(f\"{metric}: {dict_values}\")\n",
    "        elif not isinstance(value, dict):\n",
    "            # Print scalar values\n",
    "            print(f\"{metric}: {format_value(value)}\")\n",
    "\n",
    "    # Then, print nested dictionaries\n",
    "    for metric, value in metrics_results.items():\n",
    "        if isinstance(value, dict) and any(isinstance(v, dict) for v in value.values()):\n",
    "            # Print nested dictionaries\n",
    "            print(f\"\\n{metric}:\")\n",
    "            # Find the longest key length for formatting\n",
    "            max_key_length = max(len(str(k)) for k in value.keys())\n",
    "            for sub_key, sub_dict in value.items():\n",
    "                formatted_key = f\"{sub_key}:\".ljust(max_key_length + 2)\n",
    "                dict_values = \", \".join([f\"{k}: {format_value(v)}\" for k, v in sub_dict.items()])\n",
    "                print(f\"  {formatted_key} {dict_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_results(mode, epoch, batch, dataset_size, loss, start_time, batch_start_time, batch_size):\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    batch_time_ms = (current_time - batch_start_time) * 1000\n",
    "\n",
    "    current = (batch + 1) * batch_size\n",
    "    epoch_str = f\"Epoch: {epoch+1}, \" if epoch is not None else \"\"\n",
    "    \n",
    "    print(f\"\\r{mode.capitalize()} - {epoch_str}Batch: {batch+1} [{current:>5d}/{dataset_size:>5d}], \"\n",
    "          f\"Time: {elapsed_time:.0f}s {batch_time_ms:.0f}ms/step, Loss: {loss:>7f}\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(mode, model, loader, device, loss_fn, optimizer=None, epoch=None, accumulation_steps=None):\n",
    "    total_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, data in enumerate(loader, 0):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        logits, targets, loss = process_batch(model, data, device, loss_fn, mode, optimizer)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if mode == 'train':\n",
    "            if accumulation_steps is not None and (batch + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Detach from the (gradient) computation graph to save on memory\n",
    "        all_outputs.append(logits.detach())\n",
    "        all_targets.append(targets.detach())\n",
    "\n",
    "        batch_size = targets.shape[0]\n",
    "        print_batch_results(mode, epoch, batch, len(loader.dataset), loss.item(), start_time, batch_start_time, batch_size)\n",
    "\n",
    "    if mode == 'train' and optimizer is not None and accumulation_steps is not None:\n",
    "        # Ensure any remaining gradients are applied\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    all_outputs = torch.cat(all_outputs, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    print()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, all_outputs, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, loss_fn, metrics_dict, device, hyperparameters=None):\n",
    "    model.eval()\n",
    "    avg_val_loss, val_outputs, val_targets = process_batches('evaluate', model, validation_loader, device, loss_fn)\n",
    "\n",
    "    # Set default values\n",
    "    thresholds = None\n",
    "    percentile = None\n",
    "\n",
    "    # Update values based on hyperparameters if provided\n",
    "    if hyperparameters:\n",
    "        thresholds = hyperparameters.get(\"thresholds\", thresholds)\n",
    "        percentile = hyperparameters.get(\"percentile\", percentile)\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, val_targets, val_outputs, thresholds=thresholds, percentile=percentile)\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Average Loss: {avg_val_loss:.4f}\")\n",
    "    print_metrics_results(metrics_results)\n",
    "\n",
    "    return avg_val_loss, metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, training_loader, validation_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps=1):\n",
    "    print(f\"Training Epoch {epoch + 1}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    if optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss, train_outputs, train_targets = process_batches('train', model, training_loader, device, loss_fn, optimizer, epoch, accumulation_steps)\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, train_targets, train_outputs)\n",
    "\n",
    "    print(f\"Train Results:\")\n",
    "    print(f\"Average Training Loss for Epoch {epoch + 1}: {avg_train_loss:.4f}\")\n",
    "    print_metrics_results(metrics_results)\n",
    "\n",
    "    # Validation phase\n",
    "    if validation_loader is not None:\n",
    "        avg_val_loss, val_metrics_results = evaluate(model, validation_loader, loss_fn, metrics_dict, device)\n",
    "    else:\n",
    "        avg_val_loss = None\n",
    "        val_metrics_results = {}\n",
    "\n",
    "    return avg_train_loss, avg_val_loss, val_metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allenai_longformer-base-4096_Unfrozen[8, 9, 10, 11]_BCE-Balanced'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_save/allenai_longformer-base-4096_Unfrozen[8, 9, 10, 11]_BCE-Balanced_epoch_4.pth\n",
      "Loaded model training from epoch 5\n"
     ]
    }
   ],
   "source": [
    "last_saved_epoch = find_last_saved_epoch(directory=MODEL_DIRECTORY, model_name=MODEL_NAME)\n",
    "\n",
    "start_epoch = last_saved_epoch + 1 if last_saved_epoch != -1 else 0\n",
    "if last_saved_epoch != -1:\n",
    "    load_model(model, directory=MODEL_DIRECTORY, model_name=MODEL_NAME, epoch=last_saved_epoch)\n",
    "    print(f\"Loaded model training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"No saved model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate - Batch: 338 [ 7098/10805], Time: 597s 1299ms/step, Loss: 1.163739\n",
      "Evaluation Results:\n",
      "Average Loss: 0.6074\n",
      "\n",
      "Custom Classifcation Report:\n",
      "  Deviation / Discrepancy - Procedural:  binary_accuracy: 0.7578, precision: 0.8603, recall: 0.7012, f1-score: 0.7727, support: 6343.0000\n",
      "  Aircraft Equipment:                    binary_accuracy: 0.9003, precision: 0.8522, recall: 0.9104, f1-score: 0.8803, support: 4351.0000\n",
      "  Conflict:                              binary_accuracy: 0.9337, precision: 0.7394, recall: 0.9558, f1-score: 0.8338, support: 1879.0000\n",
      "  Inflight Event / Encounter:            binary_accuracy: 0.8671, precision: 0.6514, recall: 0.8762, f1-score: 0.7473, support: 2423.0000\n",
      "  ATC Issue:                             binary_accuracy: 0.8839, precision: 0.6905, recall: 0.9112, f1-score: 0.7856, support: 2522.0000\n",
      "  Deviation - Altitude:                  binary_accuracy: 0.8994, precision: 0.4671, recall: 0.9220, f1-score: 0.6201, support: 962.0000\n",
      "  Deviation - Track / Heading:           binary_accuracy: 0.8963, precision: 0.4323, recall: 0.9027, f1-score: 0.5847, support: 874.0000\n",
      "  Ground Event / Encounter:              binary_accuracy: 0.9131, precision: 0.5121, recall: 0.8415, f1-score: 0.6368, support: 978.0000\n",
      "  Flight Deck / Cabin / Aircraft Event:  binary_accuracy: 0.9144, precision: 0.4946, recall: 0.8568, f1-score: 0.6272, support: 908.0000\n",
      "  Ground Incursion:                      binary_accuracy: 0.9519, precision: 0.4629, recall: 0.9075, f1-score: 0.6131, support: 454.0000\n",
      "  Airspace Violation:                    binary_accuracy: 0.9420, precision: 0.3805, recall: 0.8670, f1-score: 0.5289, support: 406.0000\n",
      "  Deviation - Speed:                     binary_accuracy: 0.9543, precision: 0.4326, recall: 0.8663, f1-score: 0.5771, support: 389.0000\n",
      "  Ground Excursion:                      binary_accuracy: 0.9786, precision: 0.5640, recall: 0.9317, f1-score: 0.7027, support: 293.0000\n",
      "  No Specific Anomaly Occurred:          binary_accuracy: 0.9688, precision: 0.1497, recall: 0.5955, f1-score: 0.2393, support: 89.0000\n",
      "  micro avg:                             binary_accuracy: 0.9115, precision: 0.6627, recall: 0.8452, f1-score: 0.7429, support: 22871.0000\n",
      "  macro avg:                             binary_accuracy: 0.9115, precision: 0.5493, recall: 0.8604, f1-score: 0.6535, support: 22871.0000\n",
      "  weighted avg:                          binary_accuracy: 0.9115, precision: 0.7154, recall: 0.8452, f1-score: 0.7579, support: 22871.0000\n",
      "  samples avg:                           binary_accuracy: 0.9115, precision: 0.7182, recall: 0.8652, f1-score: 0.7496, support: 22871.0000\n",
      "Test Results:\n",
      "Average Loss: 0.6074\n",
      "\n",
      "Custom Classifcation Report:\n",
      "  Deviation / Discrepancy - Procedural:  binary_accuracy: 0.7578, precision: 0.8603, recall: 0.7012, f1-score: 0.7727, support: 6343.0000\n",
      "  Aircraft Equipment:                    binary_accuracy: 0.9003, precision: 0.8522, recall: 0.9104, f1-score: 0.8803, support: 4351.0000\n",
      "  Conflict:                              binary_accuracy: 0.9337, precision: 0.7394, recall: 0.9558, f1-score: 0.8338, support: 1879.0000\n",
      "  Inflight Event / Encounter:            binary_accuracy: 0.8671, precision: 0.6514, recall: 0.8762, f1-score: 0.7473, support: 2423.0000\n",
      "  ATC Issue:                             binary_accuracy: 0.8839, precision: 0.6905, recall: 0.9112, f1-score: 0.7856, support: 2522.0000\n",
      "  Deviation - Altitude:                  binary_accuracy: 0.8994, precision: 0.4671, recall: 0.9220, f1-score: 0.6201, support: 962.0000\n",
      "  Deviation - Track / Heading:           binary_accuracy: 0.8963, precision: 0.4323, recall: 0.9027, f1-score: 0.5847, support: 874.0000\n",
      "  Ground Event / Encounter:              binary_accuracy: 0.9131, precision: 0.5121, recall: 0.8415, f1-score: 0.6368, support: 978.0000\n",
      "  Flight Deck / Cabin / Aircraft Event:  binary_accuracy: 0.9144, precision: 0.4946, recall: 0.8568, f1-score: 0.6272, support: 908.0000\n",
      "  Ground Incursion:                      binary_accuracy: 0.9519, precision: 0.4629, recall: 0.9075, f1-score: 0.6131, support: 454.0000\n",
      "  Airspace Violation:                    binary_accuracy: 0.9420, precision: 0.3805, recall: 0.8670, f1-score: 0.5289, support: 406.0000\n",
      "  Deviation - Speed:                     binary_accuracy: 0.9543, precision: 0.4326, recall: 0.8663, f1-score: 0.5771, support: 389.0000\n",
      "  Ground Excursion:                      binary_accuracy: 0.9786, precision: 0.5640, recall: 0.9317, f1-score: 0.7027, support: 293.0000\n",
      "  No Specific Anomaly Occurred:          binary_accuracy: 0.9688, precision: 0.1497, recall: 0.5955, f1-score: 0.2393, support: 89.0000\n",
      "  micro avg:                             binary_accuracy: 0.9115, precision: 0.6627, recall: 0.8452, f1-score: 0.7429, support: 22871.0000\n",
      "  macro avg:                             binary_accuracy: 0.9115, precision: 0.5493, recall: 0.8604, f1-score: 0.6535, support: 22871.0000\n",
      "  weighted avg:                          binary_accuracy: 0.9115, precision: 0.7154, recall: 0.8452, f1-score: 0.7579, support: 22871.0000\n",
      "  samples avg:                           binary_accuracy: 0.9115, precision: 0.7182, recall: 0.8652, f1-score: 0.7496, support: 22871.0000\n"
     ]
    }
   ],
   "source": [
    "if start_epoch < EPOCHS:\n",
    "    print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    train_loss, val_loss, val_metrics = train(model, epoch, training_loader, testing_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps=8)\n",
    "    save_model(model, epoch, directory=MODEL_DIRECTORY, model_name=MODEL_NAME)\n",
    "    # Additional epoch-level processing if needed\n",
    "\n",
    "# Testing phase\n",
    "avg_test_loss, test_metrics_results = evaluate(model, testing_loader, loss_fn, metrics_dict, device)\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Average Loss: {avg_test_loss:.4f}\")\n",
    "print_metrics_results(test_metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_thresholds(logits, targets, metrics_dict, num_labels):\n",
    "    best_global_metric = -np.inf\n",
    "    best_thresholds = [0.5] * num_labels\n",
    "\n",
    "    # Iterate over a range of thresholds for each label\n",
    "    for label in range(num_labels):\n",
    "        for threshold in np.linspace(0, 1, 101):  # Example range and step size\n",
    "            temp_thresholds = best_thresholds.copy()\n",
    "            temp_thresholds[label] = threshold\n",
    "            metrics_results = calculate_metrics(metrics_dict, targets, logits, thresholds=temp_thresholds)\n",
    "            current_metric = metrics_results[\"Optimization Metric\"]\n",
    "\n",
    "            if current_metric > best_global_metric:\n",
    "                best_global_metric = current_metric\n",
    "                best_thresholds = temp_thresholds\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, targets, logits, thresholds=best_thresholds)\n",
    "    return best_thresholds, metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate - Batch: 338 [ 7098/10805], Time: 609s 1299ms/step, Loss: 0.507325\n"
     ]
    }
   ],
   "source": [
    "# Run the model to get logits\n",
    "_, logits, targets = process_batches('evaluate', model, testing_loader, device, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: [0.27, 0.66, 0.84, 0.71, 0.74, 0.88, 0.86, 0.85, 0.92, 0.91, 0.93, 0.93, 0.96, 0.92]\n",
      "Optimization Metric: 0.7252\n",
      "\n",
      "Custom Classifcation Report:\n",
      "  Deviation / Discrepancy - Procedural:  binary_accuracy: 0.7707, precision: 0.7721, recall: 0.8646, f1-score: 0.8157, support: 6343.0000\n",
      "  Aircraft Equipment:                    binary_accuracy: 0.9052, precision: 0.8945, recall: 0.8669, f1-score: 0.8805, support: 4351.0000\n",
      "  Conflict:                              binary_accuracy: 0.9512, precision: 0.8418, recall: 0.8861, f1-score: 0.8634, support: 1879.0000\n",
      "  Inflight Event / Encounter:            binary_accuracy: 0.8958, precision: 0.7572, recall: 0.7879, f1-score: 0.7722, support: 2423.0000\n",
      "  ATC Issue:                             binary_accuracy: 0.9078, precision: 0.7864, recall: 0.8307, f1-score: 0.8079, support: 2522.0000\n",
      "  Deviation - Altitude:                  binary_accuracy: 0.9504, precision: 0.7156, recall: 0.7349, f1-score: 0.7251, support: 962.0000\n",
      "  Deviation - Track / Heading:           binary_accuracy: 0.9504, precision: 0.6680, recall: 0.7689, f1-score: 0.7149, support: 874.0000\n",
      "  Ground Event / Encounter:              binary_accuracy: 0.9422, precision: 0.6799, recall: 0.6840, f1-score: 0.6820, support: 978.0000\n",
      "  Flight Deck / Cabin / Aircraft Event:  binary_accuracy: 0.9610, precision: 0.8048, recall: 0.7081, f1-score: 0.7534, support: 908.0000\n",
      "  Ground Incursion:                      binary_accuracy: 0.9730, precision: 0.6582, recall: 0.7423, f1-score: 0.6977, support: 454.0000\n",
      "  Airspace Violation:                    binary_accuracy: 0.9741, precision: 0.6575, recall: 0.6478, f1-score: 0.6526, support: 406.0000\n",
      "  Deviation - Speed:                     binary_accuracy: 0.9746, precision: 0.6226, recall: 0.7506, f1-score: 0.6807, support: 389.0000\n",
      "  Ground Excursion:                      binary_accuracy: 0.9896, precision: 0.7987, recall: 0.8259, f1-score: 0.8121, support: 293.0000\n",
      "  No Specific Anomaly Occurred:          binary_accuracy: 0.9805, precision: 0.2095, recall: 0.4944, f1-score: 0.2943, support: 89.0000\n",
      "  micro avg:                             binary_accuracy: 0.9376, precision: 0.7781, recall: 0.8217, f1-score: 0.7993, support: 22871.0000\n",
      "  macro avg:                             binary_accuracy: 0.9376, precision: 0.7048, recall: 0.7567, f1-score: 0.7252, support: 22871.0000\n",
      "  weighted avg:                          binary_accuracy: 0.9376, precision: 0.7834, recall: 0.8217, f1-score: 0.8010, support: 22871.0000\n",
      "  samples avg:                           binary_accuracy: 0.9376, precision: 0.8127, recall: 0.8510, f1-score: 0.7999, support: 22871.0000\n"
     ]
    }
   ],
   "source": [
    "opt_metrics_dict = {\n",
    "    \"Optimization Metric\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "}\n",
    "opt_metrics_dict.update(metrics_dict)\n",
    "\n",
    "# Optimize thresholds\n",
    "best_thresholds, metrics_results = optimize_thresholds(logits, targets, opt_metrics_dict, num_labels=len(ANOMALY_LABELS))\n",
    "\n",
    "print(\"Optimized Thresholds:\", best_thresholds)\n",
    "print_metrics_results(metrics_results)\n",
    "\n",
    "# # Use these thresholds in your evaluation\n",
    "# avg_test_loss, test_metrics_results = evaluate(model, testing_loader, loss_fn, metrics_dict, device, hyperparameters=best_thresholds)\n",
    "# print(\"Test Results with Optimized Thresholds:\")\n",
    "# print_metrics_results(test_metrics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
