{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/id2531/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_write = \"your hugging face access token\"\n",
    "login(token = access_token_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import transformers\n",
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 16:13:21.205852: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 16:13:21.205895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 16:13:21.206992: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 16:13:23.340454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dcdd30919e43ffb883f33bfccb59ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPT EXAMPLES\n",
    "f\"\"\"Consider three examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3], instances of narratives from NASA's ASRS database labeled as {label}\". Your objective is to assess if a [NEW NARRATIVE] can be assigned the same label as these examples. Return 1 if it can, and 0 otherwise.\"\"\"\n",
    "\n",
    "f\"\"\"Given threes examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3] which are instances of narratives from NASA's ASRS database labeled as \"{label}\", your task is to determine whether a [NEW NARRATIVE] can be assigned the same label as these examples. Return 1 if the [NEW NARRATIVE] can be assigned the same label as the examples and 0 otherwise.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "node"
    ]
   },
   "outputs": [],
   "source": [
    "def labeller(**kwargs):\n",
    "    \"\"\"For each Anomaly, return {Anomaly:1} and {Others:1} if the cell is empty.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if isinstance(item, str):\n",
    "            cell = item\n",
    "        if 'anomalies' in name.lower() and isinstance(item, list):\n",
    "            anomalies_list = item\n",
    "    cell_labels =[]\n",
    "    for prefix in anomalies_list:\n",
    "        if any(anomaly.strip().startswith(prefix) for anomaly in cell.split(';')):\n",
    "            cell_labels.append(prefix)\n",
    "    if cell_labels == []:\n",
    "        cell_labels = ['Others']\n",
    "    metadata = dict(zip(cell_labels, [1]*len(cell_labels)))\n",
    "    return metadata \n",
    "\n",
    "\n",
    "def load_from_df(**kwargs):\n",
    "    \"\"\"return the narratives and their Anomalies in two list.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"docs\" in name.lower():\n",
    "            source = item\n",
    "        if \"metadata\" in name.lower():\n",
    "            meta = item\n",
    "        if isinstance(item, pd.DataFrame):\n",
    "            data = item\n",
    "        if 'anomalies' in name.lower() and isinstance(item, list):\n",
    "            anomalies_list = item\n",
    "    documents = data[source].apply(lambda cell: cell.strip()).values.tolist()\n",
    "    metadata = data[meta].apply(lambda cell: labeller(input=cell, anomalies_list=anomalies_list)).values.tolist()\n",
    "    return documents, metadata\n",
    "\n",
    "def get_prompt(**kwargs):\n",
    "    \"\"\"Few shot prompt builder\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if 'example_1' in name.lower():\n",
    "            example_1 = item\n",
    "        if 'example_2' in name.lower():\n",
    "            example_2 = item\n",
    "        if 'example_3' in name.lower():\n",
    "            example_3 = item\n",
    "        if 'narrative' in name.lower():\n",
    "            narrative = item\n",
    "        if 'label' in name.lower():\n",
    "            label = item\n",
    "    system_prompt = f\"\"\"Consider three examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3], instances of narratives from NASA's ASRS database labeled as {label}\". Your objective is to assess if a [NEW NARRATIVE] can be assigned the same label as these examples. As [Answer] return 1 if it can, and 0 otherwise.\"\"\"  #  Only provide the numerical result (0 or 1) and no additional information.\n",
    "    full_prompt = f\"\"\"[INST]\n",
    "    <<SYS>>\n",
    "    {system_prompt}\n",
    "    <</SYS>>\n",
    "    \n",
    "    [EXAMPLE 1] :\n",
    "    {example_1}\n",
    "    \n",
    "    [EXAMPLE 2] :\n",
    "    {example_2}\n",
    "\n",
    "    [EXAMPLE 3] :\n",
    "    {example_3}\n",
    "    \n",
    "    [NEW NARRATIVE] :\n",
    "    {narrative}\n",
    "    \n",
    "    [/INST][Answer]\"\"\"\n",
    "    return full_prompt\n",
    "\n",
    "def format_pred(**kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"inference\" in name.lower():\n",
    "            inference = item\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item\n",
    "    pred = np.nan\n",
    "    if pattern in inference[0]['generated_text'].strip():\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    return pred, inference\n",
    "            \n",
    "\n",
    "def get_inference(**kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"query\" in name.lower():\n",
    "            query = item.strip()\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item.strip()\n",
    "        if \"task\" in name.lower():\n",
    "            task = item.strip()\n",
    "        if isinstance(item,\n",
    "                      chromadb.api.models.Collection.Collection):\n",
    "            store = item\n",
    "        if isinstance(item,\n",
    "                      transformers.pipelines.text_generation.TextGenerationPipeline):\n",
    "            pipeline = item\n",
    "    examples = store.query(query_texts=[query],\n",
    "                          include=[\"documents\"],\n",
    "                          where={task:1},\n",
    "                          n_results=3)['documents'][0]\n",
    "    prompt = get_prompt(**dict(narrative=query,\n",
    "                               example_1=examples[0],\n",
    "                               example_2=examples[1],\n",
    "                               example_3=examples[2],\n",
    "                               label=task))\n",
    "    inference = pipeline(prompt,\n",
    "                         temperature=0.01,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_length=4000)\n",
    "    pred = format_pred(inference=inference, pattern=pattern)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_per_label(y_true, y_pred):\n",
    "    correct = y_pred == y_true\n",
    "    accuracy_per_label = correct.float().mean(axis=0)\n",
    "    return accuracy_per_label\n",
    "\n",
    "def binary_accuracy_averaged(y_true, y_pred):\n",
    "    accuracy_per_label = binary_accuracy_per_label(y_true, y_pred)\n",
    "    accuracy_averaged = accuracy_per_label.mean()\n",
    "    return accuracy_averaged\n",
    "\n",
    "def custom_classification_report(y_true, y_pred):\n",
    "    report = metrics.classification_report(y_true, y_pred, output_dict=True, target_names=ANOMALY_LABELS, zero_division=0)\n",
    "    accuracy = binary_accuracy_per_label(y_true, y_pred)\n",
    "    extended_accuracy_new = np.append(accuracy, [accuracy.mean()] * (len(report) - len(accuracy)))\n",
    "\n",
    "    updated_report = {}\n",
    "    for i, class_label in enumerate(report.keys()):\n",
    "        # Create a new dictionary for the class with binary accuracy\n",
    "        class_dict = {'binary_accuracy': extended_accuracy_new[i]}\n",
    "        \n",
    "        # Merge this dictionary with the existing metrics for the class\n",
    "        class_dict.update(report[class_label])\n",
    "\n",
    "        # Update the main report dictionary\n",
    "        updated_report[class_label] = class_dict\n",
    "\n",
    "    return updated_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt(**kwargs):\n",
    "    \"\"\"Zero shot prompt builder\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if 'narrative' in name.lower():\n",
    "            narrative = item\n",
    "    system_prompt = \"\"\"<<SYS>> You will be provided with an input, which is the narrative from Aviation Safety reports of the NASA's ASRS dataset. Tell me if the input corresponds to which anomalies among the following categories: 'Deviation / Discrepancy - Procedural', 'Aircraft Equipment', 'Conflict', 'Inflight Event / Encounter', 'ATC Issue', 'Deviation - Altitude', 'Deviation - Track / Heading', 'Ground Event / Encounter', 'Flight Deck / Cabin / Aircraft Event', 'Ground Incursion', 'Airspace Violation', 'Deviation - Speed', 'Ground Excursion', 'No Specific Anomaly Occurred'. The input can correspond to one or many of them. Only return the corresponding anomalies.<<SYS>>\"\"\"\n",
    "    full_prompt = \"[INST]\" + system_prompt + \"\\n\" + narrative  + \"[/INST]\"\n",
    "    return full_prompt\n",
    "\n",
    "def format_zs_pred(**kwargs):\n",
    "    \"\"\"Parse the output of the text generation pipeline in a one-hot encoding list as predictions.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"inference\" in name.lower():\n",
    "            inference = item\n",
    "        if \"anomalies\" in name.lower():\n",
    "            anomalies_list = item\n",
    "    pattern = re.compile(r'\\[INST\\].*?\\[/INST\\]', re.DOTALL)\n",
    "    inference = re.sub(pattern, '', inference[0]['generated_text'].strip())\n",
    "    pred = [1 if label in inference else 0 for label in anomalies_list]\n",
    "    return pred\n",
    "    \n",
    "def zero_shot_inference(**kwargs):\n",
    "    \"\"\"Run inference given a narrative and return a one-hot list of present anomalies in the narrative.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"narrative\" in name.lower():\n",
    "            narrative = item.strip()\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item.strip()\n",
    "        if \"anomalies\" in name.lower():\n",
    "            anomalies = item\n",
    "        if isinstance(item,\n",
    "                      transformers.pipelines.text_generation.TextGenerationPipeline):\n",
    "            pipeline = item\n",
    "    prompt = zero_shot_prompt(**dict(narrative=narrative))\n",
    "    \n",
    "    inference = pipeline(prompt,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.01,\n",
    "                         top_k=10,\n",
    "                         num_return_sequences=1,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_length=2000)\n",
    "    pred = format_zs_pred(inference=inference, anomalies=anomalies)\n",
    "    return pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_LABELS = ['Deviation / Discrepancy - Procedural',\n",
    "                    'Aircraft Equipment',\n",
    "                    'Conflict',\n",
    "                    'Inflight Event / Encounter',\n",
    "                    'ATC Issue',\n",
    "                    'Deviation - Altitude',\n",
    "                    'Deviation - Track / Heading',\n",
    "                    'Ground Event / Encounter',\n",
    "                    'Flight Deck / Cabin / Aircraft Event',\n",
    "                    'Ground Incursion',\n",
    "                    'Airspace Violation',\n",
    "                    'Deviation - Speed',\n",
    "                    'Ground Excursion',\n",
    "                    'No Specific Anomaly Occurred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cell(cell: pd.Series, labels: list) -> pd.Series:\n",
    "    \"\"\"Encode the multilabels cell such that the cell content is replaced by \\n\n",
    "    a list of same length as labels and containing 0/1.\n",
    "\n",
    "    Args:\n",
    "        cell (pd.Series): cell containing the multilabel target\n",
    "        labels (list): actual list of labels to classify.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Expand of the cell with number of cols\\n\n",
    "        equal to number of element in labels.\n",
    "    \"\"\"\n",
    "    cell_anomalies = [item.strip() for item in cell.split(';')]\n",
    "    splited_cell_anomalies = {label: any(item.startswith(label)\n",
    "                                         for item in cell_anomalies)\n",
    "                              for label in labels}\n",
    "    return pd.Series(splited_cell_anomalies)\n",
    "\n",
    "\n",
    "def target_encoder(**kwargs):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        kwargs:\n",
    "            Datasets: kwargs should key-value of the datasets\n",
    "            target(str): All the passed datasets should contain this column.\n",
    "            That's the column to encode.\n",
    "            labels(list): list of the labels to encode.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for name, item in kwargs.items():\n",
    "        if \"target\" in name.lower():\n",
    "            target = item\n",
    "        if \"labels\" in name.lower():\n",
    "            labels = tuple(item) if isinstance(item, list) else item\n",
    "    for name, item in kwargs.items() :\n",
    "        if isinstance(item, pd.DataFrame):\n",
    "            data = item.copy()\n",
    "            encoding_series = data[target].apply(\n",
    "                lambda cell: encode_cell(cell, labels))\n",
    "            data[target] = encoding_series.values.tolist()\n",
    "            data_list.append(data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 1.55 s, total: 26.7 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_pkl = pd.read_pickle(\"./data/train_data_final.pkl\")[0]  # load pkl data\n",
    "test_data_pkl = pd.read_pickle(\"./data/test_data_final.pkl\")[0]  # load pkl test data\n",
    "train_data = train_data_pkl[['Narrative', 'Anomaly']].dropna(axis=0, subset=['Narrative','Anomaly'])\n",
    "test_data = test_data_pkl[['Narrative', 'Anomaly']].dropna(axis=0, subset=['Narrative','Anomaly'])\n",
    "train_data_enc, test_data_enc = target_encoder(train=train_data, test=test_data, target='Anomaly', labels=ANOMALY_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP THE VECTORE STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrsnlp_client = chromadb.PersistentClient(path=\"~/chromadb/asrsnlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    asrsnlp_collection = asrsnlp_client.get_collection(\"asrsnlp_collecion\")\n",
    "except ValueError:\n",
    "    embedder = DefaultEmbeddingFunction()\n",
    "    asrsnlp_collection = asrsnlp_client.get_or_create_collection(\n",
    "    name=\"asrsnlp_collection\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=embedder)\n",
    "    documents = load_from_df(df=train_data, docs='Narrative',metadata='Anomaly', anomalies=ANOMALY_LABELS)\n",
    "\n",
    "    asrsnlp_collection.add(\n",
    "        documents=documents[0][:40000],\n",
    "        metadatas=documents[1][:40000],\n",
    "        ids=[f\"ID{i}\" for i in range(0,40000)]\n",
    "    )\n",
    "\n",
    "    asrsnlp_collection.add(\n",
    "        documents=documents[0][40000:80000],\n",
    "        metadatas=documents[1][40000:80000],\n",
    "        ids=[f\"ID{i}\" for i in range(40000, 80000)]\n",
    "    )\n",
    "\n",
    "    asrsnlp_collection.add(\n",
    "        documents=documents[0][80000:],\n",
    "        metadatas=documents[1][80000:],\n",
    "        ids=[f\"ID{i}\" for i in range(80000,96986)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZERO SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 20s, sys: 1.57 s, total: 34min 21s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SINGLE INFERENCE\n",
    "result_zs = zero_shot_inference(narrative=test_data.Narrative[2],\n",
    "                             anomalies=ANOMALY_LABELS,\n",
    "                             pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ON ALL DATA IN THE TEST DATA\n",
    "preds_zs = test_data_enc.head(20).Narrative.apply(lambda cell: zero_shot_inference(narrative=cell, anomalies=ANOMALY_LABELS, pipeline=pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = test_data_enc.head(20).Anomaly.apply(lambda cell: [1 if b else 0 for b in cell])\n",
    "ytrue = np.array(ytrue.values.tolist())[:,:-1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEW SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SINGLE INFERENCE\n",
    "result_fs = get_inference(query=test_data.Narrative[1],\n",
    "                       task='No Specific Anomaly Occurred',\n",
    "                       pattern=\"[Answer]  Yes\",\n",
    "                       store=asrsnlp_collection,\n",
    "                       pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ON ALL DATA IN THE TEST DATA\n",
    "preds_fs = test_data.head(20).Narrative.apply(lambda cell: get_inference(query=cell,\n",
    "                                                                      task='No Specific Anomaly Occurred',\n",
    "                                                                      pattern=\"[Answer]  Yes\",\n",
    "                                                                      narrative=cell,\n",
    "                                                                      store=asrsnlp_collection,\n",
    "                                                                      pipeline=pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OTHERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_classif_pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
