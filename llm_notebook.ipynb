{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/id2531/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_write = \"your hugging face access token\"\n",
    "login(token = access_token_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import transformers\n",
    "import torch\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='xla', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = xm.xla_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", # specify the task for inference\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPT EXAMPLES\n",
    "f\"\"\"Consider three examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3], instances of narratives from NASA's ASRS database labeled as {label}\". Your objective is to assess if a [NEW NARRATIVE] can be assigned the same label as these examples. Return 1 if it can, and 0 otherwise.\"\"\"\n",
    "\n",
    "f\"\"\"Given threes examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3] which are instances of narratives from NASA's ASRS database labeled as \"{label}\", your task is to determine whether a [NEW NARRATIVE] can be assigned the same label as these examples. Return 1 if the [NEW NARRATIVE] can be assigned the same label as the examples and 0 otherwise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "node"
    ]
   },
   "outputs": [],
   "source": [
    "def labeller(**kwargs):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if isinstance(item, str):\n",
    "            cell = item\n",
    "        if 'anomalies' in name.lower() and isinstance(item, list):\n",
    "            anomalies_list = item\n",
    "    cell_labels =[]\n",
    "    for prefix in anomalies_list:\n",
    "        if any(anomaly.strip().startswith(prefix) for anomaly in cell.split(';')):\n",
    "            cell_labels.append(prefix)\n",
    "    if cell_labels == []:\n",
    "        cell_labels = ['Others']\n",
    "    metadata = dict(zip(cell_labels, [1]*len(cell_labels)))\n",
    "    return metadata \n",
    "\n",
    "def docextractor(**kwargs):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "    for _, item in kwargs.items():\n",
    "        if isinstance(item, str):\n",
    "            cell = item\n",
    "    doc = cell.strip()\n",
    "    return doc\n",
    "\n",
    "def load_from_df(**kwargs):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"docs\" in name.lower():\n",
    "            source = item\n",
    "        if \"metadata\" in name.lower():\n",
    "            meta = item\n",
    "        if isinstance(item, pd.DataFrame):\n",
    "            data = item\n",
    "        if 'anomalies' in name.lower() and isinstance(item, list):\n",
    "            anomalies_list = item\n",
    "    documents = data[source].apply(lambda cell: docextractor(input=cell)).values.tolist()\n",
    "    metadata = data[meta].apply(lambda cell: labeller(input=cell, anomalies_list=anomalies_list)).values.tolist()\n",
    "    return documents, metadata\n",
    "\n",
    "def get_prompt(**kwargs):\n",
    "    \"\"\"_summary_\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if 'example_1' in name.lower():\n",
    "            example_1 = item\n",
    "        if 'example_2' in name.lower():\n",
    "            example_2 = item\n",
    "        if 'example_3' in name.lower():\n",
    "            example_3 = item\n",
    "        if 'narrative' in name.lower():\n",
    "            narrative = item\n",
    "        if 'label' in name.lower():\n",
    "            label = item\n",
    "    system_prompt = f\"\"\"Consider three examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3], instances of narratives from NASA's ASRS database labeled as {label}\". Your objective is to assess if a [NEW NARRATIVE] can be assigned the same label as these examples. As [Answer] return 1 if it can, and 0 otherwise.\"\"\"  #  Only provide the numerical result (0 or 1) and no additional information.\n",
    "    full_prompt = f\"\"\"[INST]\n",
    "    <<SYS>>\n",
    "    {system_prompt}\n",
    "    <</SYS>>\n",
    "    \n",
    "    [EXAMPLE 1] :\n",
    "    {example_1}\n",
    "    \n",
    "    [EXAMPLE 2] :\n",
    "    {example_2}\n",
    "\n",
    "    [EXAMPLE 3] :\n",
    "    {example_3}\n",
    "    \n",
    "    [NEW NARRATIVE] :\n",
    "    {narrative}\n",
    "    \n",
    "    [/INST][Answer]\"\"\"\n",
    "    return full_prompt\n",
    "\n",
    "def format_pred(**kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"inference\" in name.lower():\n",
    "            inference = item\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item\n",
    "    pred = np.nan\n",
    "    if pattern in inference[0]['generated_text'].strip():\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    return pred, inference\n",
    "            \n",
    "\n",
    "def get_inference(**kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"query\" in name.lower():\n",
    "            query = item.strip()\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item.strip()\n",
    "        if \"task\" in name.lower():\n",
    "            task = item.strip()\n",
    "        if isinstance(item,\n",
    "                      chromadb.api.models.Collection.Collection):\n",
    "            store = item\n",
    "        if isinstance(item,\n",
    "                      transformers.pipelines.text_generation.TextGenerationPipeline):\n",
    "            pipeline = item\n",
    "    examples = store.query(query_texts=[query],\n",
    "                          include=[\"documents\"],\n",
    "                          where={task:1},\n",
    "                          n_results=3)['documents'][0]\n",
    "    prompt = get_prompt(**dict(narrative=query,\n",
    "                               example_1=examples[0],\n",
    "                               example_2=examples[1],\n",
    "                               example_3=examples[2],\n",
    "                               label=task))\n",
    "    inference = pipeline(prompt,\n",
    "                         temperature=1.0,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_length=4000)\n",
    "    pred = format_pred(inference=inference, pattern=pattern)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_LABELS = ['Deviation / Discrepancy - Procedural',\n",
    "                    'Aircraft Equipment',\n",
    "                    'Conflict',\n",
    "                    'Inflight Event / Encounter',\n",
    "                    'ATC Issue',\n",
    "                    'Deviation - Altitude',\n",
    "                    'Deviation - Track / Heading',\n",
    "                    'Ground Event / Encounter',\n",
    "                    'Flight Deck / Cabin / Aircraft Event',\n",
    "                    'Ground Incursion',\n",
    "                    'Airspace Violation',\n",
    "                    'Deviation - Speed',\n",
    "                    'Ground Excursion',\n",
    "                    'No Specific Anomaly Occurred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('') # load the train_data. Must contain at least Narrative and Anomaly columns. upadate the columns labels to match the labels if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"path/where/to/put/the/persitent/vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = DefaultEmbeddingFunction() # embedding model\n",
    "\n",
    "asrsnlp_collection = client.get_or_create_collection(\n",
    "    name=\"asrsnlp_collection\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=embedder)\n",
    "\n",
    "train_data = train_data.dropna(axis=0, subset=['Narrative','Anomaly'])\n",
    "documents = load_from_df(df=train_data, docs='Narrative',metadata='Anomaly', anomalies=ANOMALY_LABELS)\n",
    "\n",
    "asrsnlp_collection.add(\n",
    "    documents=documents[0][:40000],\n",
    "    metadatas=documents[1][:40000],\n",
    "    ids=[f\"ID{i}\" for i in range(0,40000)]\n",
    ")\n",
    "\n",
    "asrsnlp_collection.add(\n",
    "    documents=documents[0][40000:80000],\n",
    "    metadatas=documents[1][40000:80000],\n",
    "    ids=[f\"ID{i}\" for i in range(40000, 80000)]\n",
    ")\n",
    "\n",
    "asrsnlp_collection.add(\n",
    "    documents=documents[0][80000:],\n",
    "    metadatas=documents[1][80000:],\n",
    "    ids=[f\"ID{i}\" for i in range(80000,96986)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asrsnlp_collection = client.get_collection(\"asrsnlp_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47min 12s, sys: 5min 12s, total: 52min 25s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# run inference. adjust prompt and pattern to your need\n",
    "result = get_inference(query=test_data.Narrative[1],\n",
    "                       task='No Specific Anomaly Occurred',\n",
    "                       pattern=\"[Answer]  Yes\",\n",
    "                       store=asrsnlp_collection,\n",
    "                       pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
