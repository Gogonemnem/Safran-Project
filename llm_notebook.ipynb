{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow-probability -y\n",
    "!pip install chromadb accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"path/to/the/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/id2531/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_write = \"your hugging face access token\"\n",
    "login(token = access_token_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import transformers\n",
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 16:13:21.205852: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 16:13:21.205895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 16:13:21.206992: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 16:13:23.340454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dcdd30919e43ffb883f33bfccb59ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPT EXAMPLES\n",
    "f\"\"\"Consider three examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3], instances of narratives from NASA's ASRS database labeled as {label}\". Your objective is to assess if a [NEW NARRATIVE] can be assigned the same label as these examples. Return 1 if it can, and 0 otherwise.\"\"\"\n",
    "\n",
    "f\"\"\"Given threes examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3] which are instances of narratives from NASA's ASRS database labeled as \"{label}\", your task is to determine whether a [NEW NARRATIVE] can be assigned the same label as these examples. Return 1 if the [NEW NARRATIVE] can be assigned the same label as the examples and 0 otherwise.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f\"\"\"Consider three examples [EXAMPLE 1], [EXAMPLE 2], [EXAMPLE 3], instances of narratives from NASA's ASRS database labeled as {label}\". Your objective is to assess if a [NEW NARRATIVE] can be assigned the same label as these examples. As [Answer] return 1 if it can, and 0 otherwise.\"\"\"  #  Only provide the numerical result (0 or 1) and no additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"<<SYS>> As an aviation safety specialist, you will be provided with an input, which is the narrative from Aviation Safety reports of the NASA's ASRS dataset. Tell me if the input corresponds to which anomalies among the following categories:\n",
    "\n",
    "0. 'Deviation / Discrepancy - Procedural'\n",
    "1. 'Aircraft Equipment'\n",
    "2. 'Conflict'\n",
    "3. 'Inflight Event / Encounter'\n",
    "4. 'ATC Issue'\n",
    "5. 'Deviation - Altitude'\n",
    "6. 'Deviation - Track / Heading'\n",
    "7. 'Ground Event / Encounter'\n",
    "8. 'Flight Deck / Cabin / Aircraft Event'\n",
    "9. 'Ground Incursion'\n",
    "10. 'Airspace Violation' \n",
    "11. 'Deviation - Speed'\n",
    "12. 'Ground Excursion'\n",
    "13. 'No Specific Anomaly Occurred'\n",
    "\n",
    "Only return the corresponding anomalies as a 14-length onehot encoding list where each indice corresponds to the categorie with the same indice. For example, if the input corresponds to <<0. 'Deviation / Discrepancy - Procedural'>>, put 1 at indice 0 of the 14-length onehot encoding list like this : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]. The input can correspond to one or many of the categories, so put 1 at their respective indices and 0 elsewhere. <<SYS>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"<<SYS>> As an aviation safety specialist, you will be provided with an input, which is the narrative from Aviation Safety reports of the NASA's ASRS dataset. Tell me if the input corresponds to which anomalies among the following categories:\n",
    "\n",
    "0. 'Deviation / Discrepancy - Procedural'\n",
    "1. 'Aircraft Equipment'\n",
    "2. 'Conflict'\n",
    "3. 'Inflight Event / Encounter'\n",
    "4. 'ATC Issue'\n",
    "5. 'Deviation - Altitude'\n",
    "6. 'Deviation - Track / Heading'\n",
    "7. 'Ground Event / Encounter'\n",
    "8. 'Flight Deck / Cabin / Aircraft Event'\n",
    "9. 'Ground Incursion'\n",
    "10. 'Airspace Violation' \n",
    "11. 'Deviation - Speed'\n",
    "12. 'Ground Excursion'\n",
    "13. 'No Specific Anomaly Occurred'\n",
    "\n",
    "Only return the corresponding anomalies and nothing else. <<SYS>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"As an aviation safety specialist, you will be provided with an [NARRATIVE], which is the narrative from Aviation Safety reports of the NASA's ASRS database.\n",
    "You task is to assess if the [NARRATIVE] contains enough elements to be labelled as {label}. To help you, you will be provided 2 examples: [EXAMPLE1] is labelled as {label} and [EXAMPLE2] as {label2}. \n",
    "Return [YES] or [NO] the [NARRATIVE] has enough elements to be labelled as {label} and the probability about how much your are confident about that.\n",
    "Note that there is a total of 14 labels in the hole database.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "node"
    ]
   },
   "outputs": [],
   "source": [
    "def labeller(**kwargs):\n",
    "    \"\"\"For each Anomaly, return {Anomaly:1} and {Others:1} if the cell is empty.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if isinstance(item, str):\n",
    "            cell = item\n",
    "        if 'anomalies' in name.lower() and isinstance(item, list):\n",
    "            anomalies_list = item\n",
    "    cell_labels =[]\n",
    "    cell_notin = []\n",
    "    for prefix in anomalies_list:\n",
    "        if any(anomaly.strip().startswith(prefix) for anomaly in cell.split(';')):\n",
    "            cell_labels.append(prefix)\n",
    "        else:\n",
    "            cell_notin.append(prefix)\n",
    "    metadata = dict(zip(cell_labels + cell_notin, [1]*len(cell_labels) + [0]*len(cell_notin)))\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_from_df(**kwargs):\n",
    "    \"\"\"return the narratives and their Anomalies in two list.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"docs\" in name.lower():\n",
    "            source = item\n",
    "        if \"metadata\" in name.lower():\n",
    "            meta = item\n",
    "        if isinstance(item, pd.DataFrame):\n",
    "            data = item\n",
    "        if 'anomalies' in name.lower() and isinstance(item, list):\n",
    "            anomalies_list = item\n",
    "    documents = data[source].apply(lambda cell: cell.strip()).values.tolist()\n",
    "    metadata = data[meta].apply(lambda cell: labeller(input=cell, anomalies_list=anomalies_list)).values.tolist()\n",
    "    return documents, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fsprompt(**kwargs):\n",
    "    \"\"\"Few shot prompt builder\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if 'example1' in name.lower():\n",
    "            example1 = item\n",
    "        if 'example2' in name.lower():\n",
    "            example2 = item\n",
    "        if 'narrative' in name.lower():\n",
    "            narrative = item\n",
    "        if 'label1' in name.lower():\n",
    "            label = item\n",
    "        if 'label2' in name.lower():\n",
    "            label2 = item\n",
    "    system_prompt = f\"\"\"As an aviation safety specialist, you will be provided with an [NARRATIVE], which is the narrative from Aviation Safety reports of the NASA's ASRS database.\n",
    "You task is to assess if the [NARRATIVE] contains enough elements to be labelled as {label}. To help you, you will be provided 2 examples: [EXAMPLE1] is labelled as {label} and [EXAMPLE2] as {label2}. \n",
    "Return [YES] or [NO] the [NARRATIVE] has enough elements to be labelled as {label} and the probability about how much your are confident about that.\n",
    "Note that there is a total of 14 labels in the hole database.\"\"\"\n",
    "    full_prompt = f\"\"\"[INST]\n",
    "    <<SYS>>\n",
    "    {system_prompt}\n",
    "    <</SYS>>\n",
    "    \n",
    "    [EXAMPLE1] :\n",
    "    {example1}\n",
    "    \n",
    "    [EXAMPLE2] :\n",
    "    {example2}\n",
    "    \n",
    "    [NARRATIVE] :\n",
    "    {narrative}\n",
    "    \n",
    "    [/INST]\"\"\"\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "def format_fspred(**kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"inference\" in name.lower():\n",
    "            inference = item\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item\n",
    "    pattern_rm = re.compile(r'\\[INST\\].*?\\[/INST\\]', re.DOTALL)\n",
    "    inference_rm = re.sub(pattern_rm, '', inference[0]['generated_text'].strip())\n",
    "    pred = np.nan\n",
    "    if pattern in inference_rm:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    return pred, inference, inference_rm\n",
    "\n",
    "\n",
    "def get_fsinference(**kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"query\" in name.lower():\n",
    "            narrative = item.strip()\n",
    "        if \"pattern\" in name.lower():\n",
    "            pattern = item.strip()\n",
    "        if \"task\" in name.lower():\n",
    "            task = item.strip()\n",
    "        if isinstance(item,\n",
    "                      chromadb.api.models.Collection.Collection):\n",
    "            store = item\n",
    "        if isinstance(item,\n",
    "                      transformers.pipelines.text_generation.TextGenerationPipeline):\n",
    "            pipeline = item\n",
    "    example1 = store.query(query_texts=[narrative],\n",
    "                          include=[\"documents\"],\n",
    "                          where={task:1},\n",
    "                          n_results=1)['documents'][0][0]\n",
    "    query = store.query(query_texts=[narrative],\n",
    "                          include=[\"documents\", \"metadatas\"],\n",
    "                          where={task:0},\n",
    "                          n_results=1)\n",
    "    example2 = query['documents'][0][0]\n",
    "    label2 = list(query['metadatas'][0][0].keys())[list(query['metadatas'][0][0].values()).index(1)]\n",
    "    prompt = get_fsprompt(**dict(narrative=narrative,\n",
    "                                 example1=example1,\n",
    "                                 example2=example2,\n",
    "                                 label1=task,\n",
    "                                 label2=label2))\n",
    "    inference = pipeline(prompt,\n",
    "                         temperature=0.01,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=1,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_length=4000)\n",
    "    pred = format_fspred(inference=inference, pattern=pattern)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zsprompt(**kwargs):\n",
    "    \"\"\"Zero shot prompt builder\"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if 'narrative' in name.lower():\n",
    "            narrative = item\n",
    "        if 'prompt' in name.lower():\n",
    "            sys_prompt = item\n",
    "    system_prompt = sys_prompt\n",
    "    full_prompt = \"[INST]\" + system_prompt + \"\\n\" + narrative  + \"[/INST]\"\n",
    "    return full_prompt\n",
    "\n",
    "def format_zspred(**kwargs):\n",
    "    \"\"\"Parse the output of the text generation pipeline in a one-hot encoding list as predictions.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"inference\" in name.lower():\n",
    "            inference = item\n",
    "        if \"anomalies\" in name.lower():\n",
    "            anomalies_list = item\n",
    "    pattern = re.compile(r'\\[INST\\].*?\\[/INST\\]', re.DOTALL)\n",
    "    inference = re.sub(pattern, '', inference[0]['generated_text'].strip())\n",
    "    pred = [1 if label in inference else 0 for label in anomalies_list]\n",
    "    return pred, inference\n",
    "    \n",
    "def get_zsinference(**kwargs):\n",
    "    \"\"\"Run inference given a narrative and return a one-hot list of present anomalies in the narrative.\n",
    "    \"\"\"\n",
    "    for name, item in kwargs.items():\n",
    "        if \"narrative\" in name.lower():\n",
    "            narrative = item.strip()\n",
    "        if \"anomalies\" in name.lower():\n",
    "            anomalies = item\n",
    "        if 'prompt' in name.lower():\n",
    "            sys_prompt = item\n",
    "        if isinstance(item,\n",
    "                      transformers.pipelines.text_generation.TextGenerationPipeline):\n",
    "            pipeline = item\n",
    "    prompt = get_zsprompt(**dict(narrative=narrative,\n",
    "                                 prompt=sys_prompt))\n",
    "    inference = pipeline(prompt,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.01,\n",
    "                         top_k=10,\n",
    "                         num_return_sequences=1,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_length=2000)\n",
    "    pred = format_zspred(inference=inference, anomalies=anomalies)\n",
    "    return pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_per_label(y_true, y_pred):\n",
    "    correct = y_pred == y_true\n",
    "    accuracy_per_label = correct.float().mean(axis=0)\n",
    "    return accuracy_per_label\n",
    "\n",
    "def binary_accuracy_averaged(y_true, y_pred):\n",
    "    accuracy_per_label = binary_accuracy_per_label(y_true, y_pred)\n",
    "    accuracy_averaged = accuracy_per_label.mean()\n",
    "    return accuracy_averaged\n",
    "\n",
    "def custom_classification_report(y_true, y_pred):\n",
    "    report = metrics.classification_report(y_true, y_pred, output_dict=True, target_names=ANOMALY_LABELS, zero_division=0)\n",
    "    accuracy = binary_accuracy_per_label(y_true, y_pred)\n",
    "    extended_accuracy_new = np.append(accuracy, [accuracy.mean()] * (len(report) - len(accuracy)))\n",
    "\n",
    "    updated_report = {}\n",
    "    for i, class_label in enumerate(report.keys()):\n",
    "        # Create a new dictionary for the class with binary accuracy\n",
    "        class_dict = {'binary_accuracy': extended_accuracy_new[i]}\n",
    "        \n",
    "        # Merge this dictionary with the existing metrics for the class\n",
    "        class_dict.update(report[class_label])\n",
    "\n",
    "        # Update the main report dictionary\n",
    "        updated_report[class_label] = class_dict\n",
    "\n",
    "    return updated_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_LABELS = ['Deviation / Discrepancy - Procedural',\n",
    "                    'Aircraft Equipment',\n",
    "                    'Conflict',\n",
    "                    'Inflight Event / Encounter',\n",
    "                    'ATC Issue',\n",
    "                    'Deviation - Altitude',\n",
    "                    'Deviation - Track / Heading',\n",
    "                    'Ground Event / Encounter',\n",
    "                    'Flight Deck / Cabin / Aircraft Event',\n",
    "                    'Ground Incursion',\n",
    "                    'Airspace Violation',\n",
    "                    'Deviation - Speed',\n",
    "                    'Ground Excursion',\n",
    "                    'No Specific Anomaly Occurred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cell(cell: pd.Series, labels: list) -> pd.Series:\n",
    "    \"\"\"Encode the multilabels cell such that the cell content is replaced by \\n\n",
    "    a list of same length as labels and containing 0/1.\n",
    "\n",
    "    Args:\n",
    "        cell (pd.Series): cell containing the multilabel target\n",
    "        labels (list): actual list of labels to classify.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Expand of the cell with number of cols\\n\n",
    "        equal to number of element in labels.\n",
    "    \"\"\"\n",
    "    cell_anomalies = [item.strip() for item in cell.split(';')]\n",
    "    splited_cell_anomalies = {label: any(item.startswith(label)\n",
    "                                         for item in cell_anomalies)\n",
    "                              for label in labels}\n",
    "    return pd.Series(splited_cell_anomalies)\n",
    "\n",
    "\n",
    "def target_encoder(**kwargs):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        kwargs:\n",
    "            Datasets: kwargs should key-value of the datasets\n",
    "            target(str): All the passed datasets should contain this column.\n",
    "            That's the column to encode.\n",
    "            labels(list): list of the labels to encode.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for name, item in kwargs.items():\n",
    "        if \"target\" in name.lower():\n",
    "            target = item\n",
    "        if \"labels\" in name.lower():\n",
    "            labels = tuple(item) if isinstance(item, list) else item\n",
    "    for name, item in kwargs.items() :\n",
    "        if isinstance(item, pd.DataFrame):\n",
    "            data = item.copy()\n",
    "            encoding_series = data[target].apply(\n",
    "                lambda cell: encode_cell(cell, labels))\n",
    "            data[target] = encoding_series.values.tolist()\n",
    "            data_list.append(data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 1.55 s, total: 26.7 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_pkl = pd.read_pickle(os.path.join(WORKING_DIR,\"train_data_final.pkl\"))[0]  # load pkl data\n",
    "test_data_pkl = pd.read_pickle(os.path.join(WORKING_DIR,\"test_data_final.pkl\"))[0]  # load pkl test data\n",
    "train_data = train_data_pkl[['Narrative', 'Anomaly']].dropna(axis=0, subset=['Narrative','Anomaly'])\n",
    "test_data = test_data_pkl[['Narrative', 'Anomaly']].dropna(axis=0, subset=['Narrative','Anomaly'])\n",
    "train_data_enc, test_data_enc = target_encoder(train=train_data, test=test_data, target='Anomaly', labels=ANOMALY_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP THE VECTORE STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrsnlp_client = chromadb.PersistentClient(path=os.path.join(WORKING_DIR,\"chromadb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = DefaultEmbeddingFunction()\n",
    "asrsnlp_collection = asrsnlp_client.get_or_create_collection(name=\"asrsnlp_collection\",\n",
    "                                                             metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                                             embedding_function=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert asrsnlp_client.list_collections() != []\n",
    "except ValueError:\n",
    "    embedder = DefaultEmbeddingFunction()\n",
    "    asrsnlp_collection = asrsnlp_client.get_or_create_collection(\n",
    "    name=\"asrsnlp_collection\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=embedder)\n",
    "    documents = load_from_df(df=train_data, docs='Narrative',metadata='Anomaly', anomalies=ANOMALY_LABELS)\n",
    "\n",
    "    asrsnlp_collection.add(\n",
    "        documents=documents[0][:40000],\n",
    "        metadatas=documents[1][:40000],\n",
    "        ids=[f\"ID{i}\" for i in range(0,40000)]\n",
    "    )\n",
    "\n",
    "    asrsnlp_collection.add(\n",
    "        documents=documents[0][40000:80000],\n",
    "        metadatas=documents[1][40000:80000],\n",
    "        ids=[f\"ID{i}\" for i in range(40000, 80000)]\n",
    "    )\n",
    "\n",
    "    asrsnlp_collection.add(\n",
    "        documents=documents[0][80000:],\n",
    "        metadatas=documents[1][80000:],\n",
    "        ids=[f\"ID{i}\" for i in range(80000,96986)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZERO SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt1 = \"\"\"<<SYS>> As an aviation safety specialist, you will be provided with an input, which is the narrative from Aviation Safety reports of the NASA's ASRS dataset. Tell me if the input corresponds to which anomalies among the following categories:\n",
    "\n",
    "0. 'Deviation / Discrepancy - Procedural'\n",
    "1. 'Aircraft Equipment'\n",
    "2. 'Conflict'\n",
    "3. 'Inflight Event / Encounter'\n",
    "4. 'ATC Issue'\n",
    "5. 'Deviation - Altitude'\n",
    "6. 'Deviation - Track / Heading'\n",
    "7. 'Ground Event / Encounter'\n",
    "8. 'Flight Deck / Cabin / Aircraft Event'\n",
    "9. 'Ground Incursion'\n",
    "10. 'Airspace Violation' \n",
    "11. 'Deviation - Speed'\n",
    "12. 'Ground Excursion'\n",
    "13. 'No Specific Anomaly Occurred'\n",
    "\n",
    "Only return the corresponding anomalies and nothing else. Don't mention not corresponding anomalies in your reponse.<<SYS>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 20s, sys: 1.57 s, total: 34min 21s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SINGLE INFERENCE\n",
    "result_zs = get_zsinference(narrative=test_data.Narrative[1014798], anomalies=ANOMALY_LABELS,pipeline=pipeline, prompt=sys_prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_zs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_zs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ON ALL DATA IN THE TEST DATA\n",
    "sys_promt=\"\"\"\"\"\"\n",
    "preds_zs = test_data_enc.head(20).Narrative.apply(lambda cell: get_zsinference(narrative=cell,\n",
    "                                                                               anomalies=ANOMALY_LABELS,\n",
    "                                                                               pipeline=pipeline,\n",
    "                                                                               prompt=sys_promt)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = test_data_enc.head(20).Anomaly.apply(lambda cell: [1 if b else 0 for b in cell])\n",
    "ytrue = np.array(ytrue.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = preds_zs.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(ytrue, ypred,   target_names=ANOMALY_LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEW SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SINGLE INFERENCE\n",
    "result_fs = get_fsinference(query=test_data.Narrative[1014798], task='Conflict', pattern=\"[YES]\", store=asrsnlp_collection, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fs[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ON ALL DATA IN THE TEST DATA\n",
    "preds_fs = test_data.head(20).Narrative.apply(lambda cell: get_fsinference(query=cell,\n",
    "                                                                      task='Conflict',\n",
    "                                                                      pattern=\"[YES]\",\n",
    "                                                                      store=asrsnlp_collection,\n",
    "                                                                      pipeline=pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = preds_zs.values.tolist()\n",
    "print(metrics.classification_report(ytrue, ypred,   target_names=ANOMALY_LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OTHERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_classif_pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
