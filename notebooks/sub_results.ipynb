{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_LABELS = ['Deviation / Discrepancy - Procedural',\n",
    "                    'Aircraft Equipment',\n",
    "                    'Conflict',\n",
    "                    'Inflight Event / Encounter',\n",
    "                    'ATC Issue',\n",
    "                    'Deviation - Altitude',\n",
    "                    'Deviation - Track / Heading',\n",
    "                    'Ground Event / Encounter',\n",
    "                    'Flight Deck / Cabin / Aircraft Event',\n",
    "                    'Ground Incursion',\n",
    "                    'Airspace Violation',\n",
    "                    'Deviation - Speed',\n",
    "                    'Ground Excursion',\n",
    "                    'No Specific Anomaly Occurred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../out/results-sub.csv')\n",
    "subset = pd.read_parquet('../out/subset_test_data.parquet')\n",
    "fs_pred = joblib.load('../out/ypred_fs0.joblib')\n",
    "zs_pred = joblib.load('../out/ypred_p5_zs_abv.joblib')  # joblib.load('../out/ypred_p5_zs.joblib') to load the inferences results with abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.array(subset.Anomaly.values.tolist())\n",
    "l_values = list(classification_report(ytrue, zs_pred, target_names=ANOMALY_LABELS, output_dict=True).values())\n",
    "l_keys = list(classification_report(ytrue, zs_pred, target_names=ANOMALY_LABELS, output_dict=True).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = [item['f1-score'] for item in l_values]\n",
    "llama_results = pd.DataFrame({'Model Name' : ['LLAMA-2-7B']*len(l_values), 'Category' : l_keys, 'f1-score': f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = results[llama_results.columns]\n",
    "results_full = pd.concat([results_full, llama_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'bert':'BERT',\n",
    "          'nasa':'SafeAeroBERT',\n",
    "          'alle': 'Longformer',\n",
    "          'aero': 'AeroBOT',\n",
    "          'llam':'LLAMA-2-7B'}\n",
    "short_name = ['BertBase_Unfrz_BCE',\n",
    "              'SafeAeroBERT_Unfrz_BCE',\n",
    "              'Longformer_Unfrz_BCE',\n",
    "              'AeroBOT_BertBase_Unfrz_BCE',\n",
    "              'LLAMA-2-7B']\n",
    "cat_exclude = ['No Specific Anomaly Occurred', 'micro avg', 'macro avg', 'weighted avg', 'samples avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette()\n",
    "color_palette = sns.color_palette()\n",
    "model_order = ['SafeAeroBERT_Unfrz_BCE', 'Longformer_Unfrz_BCE', 'AeroBOT_BertBase_Unfrz_BCE', 'BertBase_Unfrz_BCE', 'LLAMA-2-7B']\n",
    "model_order_bis = ['SafeAeroBERT', 'Longformer', 'AeroBOT', 'BERT', 'LLAMA-2-7B']\n",
    "model_palette = {model: color_palette[index % len(color_palette)] for index, model in enumerate(model_order)}\n",
    "model_palette_bis = {model: color_palette[index % len(color_palette)] for index, model in enumerate(model_order_bis)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full['Model'] = results_full['Model Name'].apply(lambda cell: models[cell[:4].lower()])\n",
    "map = dict(zip(results_full['Model Name'].unique().tolist(), short_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full['Model Name'] = results_full['Model Name'].apply(lambda cell: map[cell])\n",
    "results_full = results_full.sort_values(by='f1-score', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZERO SHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W/ FORMAT ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytrue, np.array(zs_pred), target_names=ANOMALY_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_theme(style=\"ticks\")\n",
    "plt.grid()\n",
    "sns.stripplot(data=results_full[results_full['Category'].isin(['macro avg'])], x=\"Model Name\", y=\"f1-score\", color='yellow', size=10, jitter=True)\n",
    "sns.violinplot(data=results_full[~results_full['Category'].isin(cat_exclude)], x=\"Model Name\", y=\"f1-score\", inner=\"points\", saturation=0.75, palette=model_palette)\n",
    "plt.title('Violin Plot of F1-scores by best Unfrozen Model Configurations with LLAMA-2-7B (ABV)')\n",
    "for i in range(len(results_full[results_full['Category'].isin(['macro avg'])])):\n",
    "    plt.text(results_full[results_full['Category'].isin(['macro avg'])].iloc[i]['Model Name'], results_full[results_full['Category'].isin(['macro avg'])].iloc[i]['f1-score'] + 0.02, \n",
    "             f\"{results_full[results_full['Category'].isin(['macro avg'])].iloc[i]['f1-score']:.3f}\", ha='center', va='bottom', fontsize=9, color='yellow')\n",
    "\n",
    "plt.axvline(x=1, color='red', linestyle='--')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation = 45, ha='right', fontsize=9)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WO FORMAT ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_idx = np.where(np.sum(zs_pred, axis=1)==0)[0]\n",
    "ytrue_gf = ytrue[~errors_idx]\n",
    "zs_pred_gf = np.array(zs_pred)[~errors_idx]\n",
    "l_gf_values = list(classification_report(ytrue_gf, zs_pred_gf, target_names=ANOMALY_LABELS, output_dict=True).values())\n",
    "l_gf_keys = list(classification_report(ytrue_gf, zs_pred_gf, target_names=ANOMALY_LABELS, output_dict=True).keys())\n",
    "f1score = [item['f1-score'] for item in l_gf_values]\n",
    "llama_gf_results = pd.DataFrame({'Model Name' : ['LLAMA-2-7B']*len(l_gf_values), 'Category' : l_gf_keys, 'f1-score': f1score})\n",
    "results_gf = results[llama_gf_results.columns]\n",
    "results_gf = pd.concat([results_gf, llama_gf_results])\n",
    "results_gf['Model'] = results_gf['Model Name'].apply(lambda cell: models[cell[:4].lower()])\n",
    "map = dict(zip(results_gf['Model Name'].unique().tolist(), short_name))\n",
    "results_gf['Model Name'] = results_gf['Model Name'].apply(lambda cell: map[cell])\n",
    "results_gf = results_gf.sort_values(by='f1-score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytrue_gf, np.array(zs_pred_gf), target_names=ANOMALY_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_theme(style=\"ticks\")\n",
    "plt.grid()\n",
    "sns.stripplot(data=results_gf[results_gf['Category'].isin(['macro avg'])], x=\"Model Name\", y=\"f1-score\", color='yellow', size=10, jitter=True)\n",
    "sns.violinplot(data=results_gf[~results_gf['Category'].isin(cat_exclude)], x=\"Model Name\", y=\"f1-score\",\n",
    "               inner=\"points\", saturation=0.75, palette=model_palette)\n",
    "plt.title('Violin Plot of F1-scores by best Unfrozen Model Configurations with LLAMA-2-7B')\n",
    "for i in range(len(results_gf[results_gf['Category'].isin(['macro avg'])])):\n",
    "    plt.text(results_gf[results_gf['Category'].isin(['macro avg'])].iloc[i]['Model Name'], results_gf[results_gf['Category'].isin(['macro avg'])].iloc[i]['f1-score'] + 0.02, \n",
    "             f\"{results_gf[results_gf['Category'].isin(['macro avg'])].iloc[i]['f1-score']:.3f}\", ha='center', va='bottom', fontsize=9, color='yellow')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation = 45, ha='right', fontsize=9)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEW SHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W/ FORMAT ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytrue[:,0], np.array(fs_pred)[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WO FORMAT ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_format_errors = np.where(np.sum(np.array(fs_pred), axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytrue[:,0][~fs_format_errors], np.array(fs_pred)[:,0][~fs_format_errors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_fs_f1_score = f1_score(ytrue[:,0][~fs_format_errors], np.array(fs_pred)[:,0][~fs_format_errors])\n",
    "fs_f1_score = f1_score(ytrue[:,0], np.array(fs_pred)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=[fs_f1_score, gf_fs_f1_score], x=['With Format Errors', 'Without Format Errors'], width=0.2)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Scores for Category <<Deviation - Procedural / Discreperancy>>')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMAT ERRORS PERCENTAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_fe_per = (len(np.where(np.sum(np.array(fs_pred), axis=1)==0)[0])/len(np.array(fs_pred)))*100\n",
    "zs_fe_per = (len(np.where(np.sum(np.array(zs_pred), axis=1)==0)[0])/len(np.array(zs_pred)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=[zs_fe_per, fs_fe_per], x=['Zero Shot Prompting', 'Few Shot Prompting'], width=0.2)\n",
    "plt.ylabel('%')\n",
    "plt.title('Percentage of Format Errors by Prompting Strategy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBSET DISTRIBUTION PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x = ANOMALY_LABELS, height= np.sum(ytrue, axis=0), color='r', alpha=0.6, width=0.4)\n",
    "plt.xticks(rotation = 45, ha='right', fontsize=9)\n",
    "plt.title('Grouped anomalies types in the subset of the Test Set')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
